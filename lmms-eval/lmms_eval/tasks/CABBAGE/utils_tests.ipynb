{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import openai\n",
    "import requests\n",
    "import yaml\n",
    "from loguru import logger as eval_logger\n",
    "from openai import OpenAI\n",
    "\n",
    "NUM_SECONDS_TO_SLEEP = 5\n",
    "\n",
    "MULTIPLE_CHOICE_PROMPT = '\\nAnswer with the option\\'s letter from the given choices directly.'\n",
    "OPEN_ENDED_PROMPT = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dev split: 16 examples [00:00, 760.62 examples/s]\n",
      "Generating validation split: 4067 examples [00:02, 1649.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "task_name = 'PlantNet_Multi_Identification'\n",
    "folder = '/workdir/important_datasets/AGRIVQA/'\n",
    "dataset_name = \"parquet\"\n",
    "data_files = {\n",
    "    \"dev\": folder+task_name+\"/dev-00000-of-00001.parquet\",\n",
    "    #\"test\": folder+task_name+\"/test-00000-of-00001.parquet\",\n",
    "    \"validation\": folder+task_name+\"/validation-00000-of-00001.parquet\"\n",
    "}\n",
    "split = \"dev\"\n",
    "\n",
    "dataset = load_dataset(dataset_name,data_files=data_files, split=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "task_name = 'PlantNet_Multi_Identification'\n",
    "folder = '/workdir/important_datasets/AGRIVQA/'\n",
    "dataset_name = \"parquet\"\n",
    "data_files = {\n",
    "    \"dev\": folder+task_name+\"/dev-00000-of-00001.parquet\",\n",
    "    #\"test\": folder+task_name+\"/test-00000-of-00001.parquet\",\n",
    "    \"validation\": folder+task_name+\"/validation-00000-of-00001.parquet\"\n",
    "}\n",
    "split = \"dev\"\n",
    "\n",
    "dataset = load_dataset(dataset_name,data_files=data_files, split=split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, the .yaml file is read and converted into a dictionary named config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "__file__= \"/workdir/lmms-eval/lmms_eval/tasks/CABBAGE/CABBAGE.yaml\"\n",
    "\n",
    "#rule_dict = json.load(open(os.path.join(os.path.dirname(os.path.abspath(__file__)), \"rule.json\"), \"r\"))\n",
    "\n",
    "with open(Path(__file__).parent / \"CABBAGE.yaml\", \"r\") as f:\n",
    "    raw_data = f.readlines()\n",
    "    safe_data = []\n",
    "    for i, line in enumerate(raw_data):\n",
    "        # remove function definition since yaml load cannot handle it\n",
    "        if \"!function\" not in line:\n",
    "            safe_data.append(line)\n",
    "\n",
    "    config = yaml.safe_load(\"\".join(safe_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CABBAGE_AgriExam'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['task'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'group': 'CABBAGE',\n",
       " 'task': ['CABBAGE_AgriExam', 'CABBAGE_Agri500P', 'CABBAGE_wikiHow'],\n",
       " 'metadata': {'version': 0.0}}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, as with all other tasks, the visuals and text are prepared for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_images_tokens(input_string):\n",
    "    for i in range(1, 8):\n",
    "        question_text = f\"<image {i}>\"\n",
    "        query_text = \"<image>\"\n",
    "        if question_text in input_string:\n",
    "            input_string = input_string.replace(question_text, query_text)\n",
    "    return input_string\n",
    "\n",
    "def parse_options(options):\n",
    "    option_letters = [chr(ord(\"A\") + i) for i in range(len(options))]\n",
    "    choices_str = \"\\n\".join([f\"{option_letter}. {option}\" for option_letter, option in zip(option_letters, options)])\n",
    "    return choices_str\n",
    "\n",
    "def CABBAGE_doc_to_text(doc, lmms_eval_specific_kwargs=None):\n",
    "    question = replace_images_tokens(doc['question'])\n",
    "    if lmms_eval_specific_kwargs is None:\n",
    "        lmms_eval_specific_kwargs = {}\n",
    "    post_prompt = ''\n",
    "    if doc['question_type']=='multiple-choice':\n",
    "        post_prompt = MULTIPLE_CHOICE_PROMPT\n",
    "    elif doc['question_type']=='open-ended':\n",
    "        post_prompt = OPEN_ENDED_PROMPT\n",
    "        \n",
    "    pre_prompt=''\n",
    "    if doc.get('context'):\n",
    "        pre_prompt = f'Context: {doc.get('context')}\\n'\n",
    "\n",
    "    if doc.get('options'):\n",
    "        options = parse_options(doc['options'])\n",
    "        return f\"{pre_prompt}Question: {question}\\n\\nOptions:\\n{options}\\n{post_prompt}\"\n",
    "    return f\"{pre_prompt}Question: {question}{post_prompt}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'dev_PlantNet_Multi_Identification_1',\n",
       " 'question': 'What is the scientific name of the plant shown in <image 1>?',\n",
       " 'options': [],\n",
       " 'explanation': '',\n",
       " 'image_1': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=675x900>,\n",
       " 'image_2': None,\n",
       " 'image_3': None,\n",
       " 'image_4': None,\n",
       " 'image_5': None,\n",
       " 'img_type': ['Picture'],\n",
       " 'answer': 'Borago officinalis',\n",
       " 'topic_difficulty': 5,\n",
       " 'question_type': 'open-ended',\n",
       " 'subfield': 'scientific_name_5',\n",
       " 'metadata': {'author': 'PlantNet',\n",
       "  'eppo_code': ['BOROF'],\n",
       "  'event_date': ['2021-04-29T14:34:32Z'],\n",
       "  'gbif_id': [3949249620],\n",
       "  'gbif_key': [2926110],\n",
       "  'kingdom': 'Plantae',\n",
       "  'language': 'English',\n",
       "  'license': 'CC BY-SA 4.0',\n",
       "  'region': ['Europe'],\n",
       "  'source': 'PlantNet',\n",
       "  'tag': ['habit'],\n",
       "  'url': ['https://bs.plantnet.org/image/o/06437fe7f3d1e038e4179e2b8ca30f878c97e317']}}"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the scientific name of the plant shown in <image>?\n"
     ]
    }
   ],
   "source": [
    "doc = dataset[0]\n",
    "print(CABBAGE_doc_to_text(doc)) #config['task'][0]['lmms_eval_specific_kwargs']['default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CABBAGE_doc_to_visual(doc):\n",
    "    visual = []\n",
    "    for i in range(1,6):\n",
    "        if doc.get(f'image_{i}'):\n",
    "            visual.append(doc[f'image_{i}'].convert(\"RGB\"))\n",
    "    return visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'dev_PlantNet_Multi_Identification_1',\n",
       " 'question': 'What is the scientific name of the plant shown in <image 1>?',\n",
       " 'options': [],\n",
       " 'explanation': '',\n",
       " 'image_1': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=675x900>,\n",
       " 'image_2': None,\n",
       " 'image_3': None,\n",
       " 'image_4': None,\n",
       " 'image_5': None,\n",
       " 'img_type': ['Picture'],\n",
       " 'answer': 'Borago officinalis',\n",
       " 'topic_difficulty': 5,\n",
       " 'question_type': 'open-ended',\n",
       " 'subfield': 'scientific_name_5',\n",
       " 'metadata': {'author': 'PlantNet',\n",
       "  'eppo_code': ['BOROF'],\n",
       "  'event_date': ['2021-04-29T14:34:32Z'],\n",
       "  'gbif_id': [3949249620],\n",
       "  'gbif_key': [2926110],\n",
       "  'kingdom': 'Plantae',\n",
       "  'language': 'English',\n",
       "  'license': 'CC BY-SA 4.0',\n",
       "  'region': ['Europe'],\n",
       "  'source': 'PlantNet',\n",
       "  'tag': ['habit'],\n",
       "  'url': ['https://bs.plantnet.org/image/o/06437fe7f3d1e038e4179e2b8ca30f878c97e317']}}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<PIL.Image.Image image mode=RGB size=675x900>]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CABBAGE_doc_to_visual(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CABBAGE_process_results_exact_match(doc, results):\n",
    "    # I know this is weird, but it's how llava parse it.\n",
    "    target = doc['answer'].strip().lower()\n",
    "    pred = results[0].strip().lower()\n",
    "    if pred == target:\n",
    "        return {\"exact_match\": 1.0}\n",
    "    # pattern: ^[A-Z]\\. .*\n",
    "    if len(pred) >= 2 and pred[0].isupper() and pred[1] == \".\":\n",
    "        result = 1.0 if pred[0] == target else 0.0\n",
    "        return {\"exact_match\": result}\n",
    "    return {\"exact_match\": 0.0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_EVAL_MODEL_NAME = \"gpt-4o-mini\"#config[\"metadata\"][\"gpt_eval_model_name\"]\n",
    "\n",
    "API_TYPE = 'openai' #os.getenv(\"API_TYPE\", \"openai\")\n",
    "\n",
    "if API_TYPE == \"openai\":\n",
    "    API_URL = os.getenv(\"OPENAI_API_URL\", \"https://api.openai.com/v1/chat/completions\")\n",
    "    API_KEY = os.getenv(\"OPENAI_API_KEY\", \"YOUR_API_KEY\")\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "elif API_TYPE == \"azure\":\n",
    "    API_URL = os.getenv(\"AZURE_ENDPOINT\", \"https://api.cognitive.microsoft.com/sts/v1.0/issueToken\")\n",
    "    API_KEY = os.getenv(\"AZURE_API_KEY\", \"YOUR_API_KEY\")\n",
    "    headers = {\n",
    "        \"api-key\": API_KEY,\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'dev_PlantNet_Multi_Identification_1',\n",
       " 'question': 'What is the scientific name of the plant shown in <image 1>?',\n",
       " 'options': [],\n",
       " 'explanation': '',\n",
       " 'image_1': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=675x900>,\n",
       " 'image_2': None,\n",
       " 'image_3': None,\n",
       " 'image_4': None,\n",
       " 'image_5': None,\n",
       " 'img_type': ['Picture'],\n",
       " 'answer': 'Borago officinalis',\n",
       " 'topic_difficulty': 5,\n",
       " 'question_type': 'open-ended',\n",
       " 'subfield': 'scientific_name_5',\n",
       " 'metadata': {'author': 'PlantNet',\n",
       "  'eppo_code': ['BOROF'],\n",
       "  'event_date': ['2021-04-29T14:34:32Z'],\n",
       "  'gbif_id': [3949249620],\n",
       "  'gbif_key': [2926110],\n",
       "  'kingdom': 'Plantae',\n",
       "  'language': 'English',\n",
       "  'license': 'CC BY-SA 4.0',\n",
       "  'region': ['Europe'],\n",
       "  'source': 'PlantNet',\n",
       "  'tag': ['habit'],\n",
       "  'url': ['https://bs.plantnet.org/image/o/06437fe7f3d1e038e4179e2b8ca30f878c97e317']}}"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_dict = json.load(open(os.path.join(os.path.dirname(os.path.abspath(__file__)), \"rule.json\"), \"r\"))\n",
    "\n",
    "def get_eval(content: str, max_tokens: int, retries: int = 5):\n",
    "    global headers\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful and precise agronomy assistant for checking the quality of the answer.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": content},\n",
    "    ]\n",
    "\n",
    "    payload = {\n",
    "        \"model\": GPT_EVAL_MODEL_NAME,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.2,\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "\n",
    "    if API_TYPE == \"azure\":\n",
    "        payload.pop(\"model\")\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.post(API_URL, headers=headers, json=payload, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            response_data = response.json()\n",
    "\n",
    "            content = response_data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "            if content != \"\":\n",
    "                return content, response_data[\"model\"]\n",
    "            break  # If successful, break out of the loop\n",
    "\n",
    "        except Exception as e:\n",
    "            eval_logger.info(f\"Attempt {attempt + 1} failed with error: {e}\")\n",
    "            if attempt < retries:  # If we have retries left, sleep and then continue to next attempt\n",
    "                time.sleep(NUM_SECONDS_TO_SLEEP)\n",
    "            else:  # If this was the last attempt, log and return empty\n",
    "                eval_logger.error(f\"All {retries} attempts failed. Last error message: {e}\")\n",
    "                return \"\", \"\"\n",
    "    return \"\", \"\"\n",
    "\n",
    "\n",
    "def parse_score(review):\n",
    "    score = review.split(\"\\n\")[0]\n",
    "    score = score.replace(\",\", \" \")\n",
    "    try:\n",
    "        return float(score)\n",
    "    except ValueError:\n",
    "        eval_logger.debug(f\"Score not parsed: {review}. Returning -1\")\n",
    "        return -1\n",
    "\n",
    "def CABBAGE_process_results_gpt_eval(doc, results):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        doc: a instance of the eval dataset\n",
    "        results: [pred]\n",
    "    Returns:\n",
    "        a dictionary with key: metric name (in this case coco_bleu), value: metric value\n",
    "    \"\"\"\n",
    "    question = doc.get(\"question\", \"\")\n",
    "    ans1 = doc.get(\"answer\", \"\")\n",
    "    ans2 = results[0] if results else \"\"\n",
    "    \n",
    "    if doc['question_type']=='multiple-choice':\n",
    "        exact_match_result = CABBAGE_process_results_exact_match(doc, results)\n",
    "        review_dict = {'CABBAGE_gpt_eval': {\"question\": question, \"ans1\": ans1, \"ans2\": ans2, \"difficulty\": doc.get('options_difficulty'), \"review\": '',  \"score\": exact_match_result['exact_match']*10, \"eval_model\": '', \"content\": ''},\n",
    "                       'exact_match': exact_match_result['exact_match']}\n",
    "        return review_dict\n",
    "    \n",
    "    try:\n",
    "        question = doc.get(\"question\", \"\")\n",
    "        ans1 = doc.get(\"answer\", \"\")\n",
    "        role1= \"Expert\"\n",
    "        ans2 = results[0] if results else \"\"\n",
    "        role2 = rule_dict.get(\"role\", \"user\")\n",
    "        \n",
    "        captions = doc.get(\"caption\", [])\n",
    "        # TODO add the context label to the dataset (docs)\n",
    "        #context = config['lmms_eval_specific_kwargs']['default']['context_prompt'].format(book_title=book_title, chapter_title=chapter_title)\n",
    "        context=''\n",
    "        if doc.get('category'):\n",
    "            context += f'Category: {doc.get('category')}\\n'\n",
    "        if doc.get('context'):\n",
    "            context += f'Context: {doc.get('context')}\\n'\n",
    "        \n",
    "        prompt = rule_dict.get(\"prompt\", \"\")\n",
    "        content = f\"[Context]\\n{context}\\n\" f\"[Question]\\n{question}\\n\\n\" f\"[{role1}]\\n{ans1}\\n\\n[End of {role1}]\\n\\n\" f\"[{role2}]\\n{ans2}\\n\\n[End of {role2}]\\n\\n\" f\"[System]\\n{prompt}\\n\\n\"\n",
    "        review, model_name = get_eval(content, 1024)\n",
    "        score = parse_score(review)\n",
    "    except Exception as e:\n",
    "        eval_logger.error(f\"Error for Question ID: {doc.get('question_id', 'Unknown')}: {e}\")\n",
    "        review = \"Failed to Get a Proper Review.\"\n",
    "        model_name = \"Failed Request\"\n",
    "        score = -1\n",
    "    if score > 6:\n",
    "        value = 1.0\n",
    "    else:\n",
    "        value = 0.0\n",
    "        \n",
    "    review_dict = {'CABBAGE_gpt_eval': {\"question\": question, \"ans1\": ans1, \"ans2\": ans2, \"difficulty\": doc.get('options_difficulty'), \"review\": review, \"score\": score, \"eval_model\": model_name, \"content\": content},\n",
    "                   'exact_match' : value}\n",
    "\n",
    "    return review_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=CABBAGE_process_results_gpt_eval(dataset[1],['Prova'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Context]\n",
      "Category: Nutrition, Health Benefits, and Food Processing\n",
      "Context: This question is sourced from the book titled 'Peanut', specifically found in the chapter 'Quality and Safety of Foods Derived from Peanut'.\n",
      "\n",
      "\n",
      "[Question]\n",
      "Can contaminated grains be used to feed animals?\n",
      "\n",
      "[Expert]\n",
      "No. The carcinogenic effects are also observed in birds, mammals, and fish. Therefore, it is necessary to be cautious when feeding livestock with peanut cake.\n",
      "According to Ordinance No. 7, dated 9/11/1988, from the MAPA (BRASIL, 1988), animal feed ingredients must contain, at most, 50 ppb or 50 Âµg/kg of aflatoxins (B_1+B_2+G_1+G_2).\n",
      "\n",
      "[End of Expert]\n",
      "\n",
      "[Assistant]\n",
      "Prova\n",
      "\n",
      "[End of Assistant]\n",
      "\n",
      "[System]\n",
      "We would like to request your feedback on the performance of an AI assistant in response to the user question displayed above, which pertains to agronomic knowledge. Please compare the assistant's response to a provided expert response to assess alignment and accuracy.\n",
      "Rate the assistant's performance based on the following criteria:\n",
      "1. Accuracy in replicating the information provided in the expert response regarding agronomic practices and principles.\n",
      "2. Relevance to the specific agronomic context or query posed by the user.\n",
      "3. Assess the assistant's ability to identify key agronomic challenges, address specific crop needs, and predict outcomes effectively.\n",
      "4. Level of detail and clarity in the explanation of agronomic concepts.\n",
      "Provide an overall score on a scale of 1 to 10, where a higher score indicates better alignment with the expert response and overall performance.\n",
      "Please first output a single line containing only the numerical score. In the subsequent line, provide a comprehensive explanation of your evaluation, ensuring objectivity and clarity in your judgment, and specify any notable divergences or alignments observed in comparison to the expert response.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result['CABBAGE_gpt_eval']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_eval_aggregation(results):\n",
    "    try:\n",
    "        scores = []\n",
    "        for result in results:\n",
    "            if result[\"score\"] == -1:\n",
    "                continue\n",
    "            scores.append(result[\"score\"])\n",
    "\n",
    "        stats = np.asarray(scores).mean(0).tolist()\n",
    "        stats = round(stats, 3)\n",
    "        return stats*10\n",
    "    except Exception as e:\n",
    "        eval_logger.info(f\"Error in aggregation: {e}\")\n",
    "        return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
