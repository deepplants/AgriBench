{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import openai\n",
    "import requests\n",
    "import yaml\n",
    "from loguru import logger as eval_logger\n",
    "from openai import OpenAI\n",
    "\n",
    "NUM_SECONDS_TO_SLEEP = 5 # This defines the number of seconds to wait between retries when attempting to contact the OpenAI server.\n",
    "\n",
    "LLAVA_W_METRICS = [\"gpt_eval_llava_conv\", \"gpt_eval_llava_detail\", \"gpt_eval_llava_complex\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"lmms-lab/llava-bench-in-the-wild\"\n",
    "split = \"train\"\n",
    "\n",
    "dataset = load_dataset(dataset_name, split=split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, the .yaml file is read and converted into a dictionary named config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__file__= \"/workdir/lmms-eval/lmms_eval/tasks/llava-in-the-wild/utils.ipynb\"\n",
    "\n",
    "rule_dict = json.load(open(os.path.join(os.path.dirname(os.path.abspath(__file__)), \"rule.json\"), \"r\"))\n",
    "\n",
    "with open(Path(__file__).parent / \"llava-in-the-wild.yaml\", \"r\") as f:\n",
    "    raw_data = f.readlines()\n",
    "    safe_data = []\n",
    "    for i, line in enumerate(raw_data):\n",
    "        # remove function definition since yaml load cannot handle it\n",
    "        if \"!function\" not in line:\n",
    "            safe_data.append(line)\n",
    "\n",
    "    config = yaml.safe_load(\"\".join(safe_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, the variables for GPT-Eval are defined and assigned: MODEL_NAME, API_URL, and API_KEY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_EVAL_MODEL_NAME = config[\"metadata\"][\"gpt_eval_model_name\"]\n",
    "\n",
    "API_TYPE = os.getenv(\"API_TYPE\", \"openai\")\n",
    "\n",
    "if API_TYPE == \"openai\":\n",
    "    API_URL = os.getenv(\"OPENAI_API_URL\", \"https://api.openai.com/v1/chat/completions\")\n",
    "    API_KEY = os.getenv(\"OPENAI_API_KEY\", \"YOUR_API_KEY\")\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "elif API_TYPE == \"azure\":\n",
    "    API_URL = os.getenv(\"AZURE_ENDPOINT\", \"https://api.cognitive.microsoft.com/sts/v1.0/issueToken\")\n",
    "    API_KEY = os.getenv(\"AZURE_API_KEY\", \"YOUR_API_KEY\")\n",
    "    headers = {\n",
    "        \"api-key\": API_KEY,\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, as with all other tasks, the visuals and text are prepared for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llava_doc_to_visual(doc):\n",
    "    return [doc[\"image\"].convert(\"RGB\")]\n",
    "\n",
    "\n",
    "def llava_doc_to_text(doc, lmms_eval_specific_kwargs=None):\n",
    "    if lmms_eval_specific_kwargs is None:\n",
    "        lmms_eval_specific_kwargs = {}\n",
    "    pre_prompt = lmms_eval_specific_kwargs.get(\"pre_prompt\", \"\")\n",
    "    post_prompt = lmms_eval_specific_kwargs.get(\"post_prompt\", \"\")\n",
    "    return f\"{pre_prompt}{doc['question']}{post_prompt}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Discuss how this creative twist on a classic work of art might be interpreted differently by various audiences.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llava_doc_to_text(dataset[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_eval() is the function responsible for contacting the OpenAI server and returning the evaluation.\n",
    "#### parse_score() extracts the response and returns the scores for the two answers: gpt_answer_score and llava_answer_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval(content: str, max_tokens: int, retries: int = 5):\n",
    "    global headers\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful and precise assistant for checking the quality of the answer.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": content},\n",
    "    ]\n",
    "\n",
    "    payload = {\n",
    "        \"model\": GPT_EVAL_MODEL_NAME,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.2,\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "\n",
    "    if API_TYPE == \"azure\":\n",
    "        payload.pop(\"model\")\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.post(API_URL, headers=headers, json=payload, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            response_data = response.json()\n",
    "\n",
    "            content = response_data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "            if content != \"\":\n",
    "                return content, response_data[\"model\"]\n",
    "            break  # If successful, break out of the loop\n",
    "\n",
    "        except Exception as e:\n",
    "            eval_logger.info(f\"Attempt {attempt + 1} failed with error: {e}\")\n",
    "            if attempt < retries:  # If we have retries left, sleep and then continue to next attempt\n",
    "                time.sleep(NUM_SECONDS_TO_SLEEP)\n",
    "            else:  # If this was the last attempt, log and return empty\n",
    "                eval_logger.error(f\"All {retries} attempts failed. Last error message: {e}\")\n",
    "                return \"\", \"\"\n",
    "    return \"\", \"\"\n",
    "\n",
    "\n",
    "def parse_score(review):\n",
    "    try:\n",
    "        score_pair = review.split(\"\\n\")[0]\n",
    "        score_pair = score_pair.replace(\",\", \" \")\n",
    "        sp = score_pair.split(\" \")\n",
    "        if len(sp) == 2:\n",
    "            return [float(sp[0]), float(sp[1])]\n",
    "        else:\n",
    "            eval_logger.debug(f\"Can not split: {review}. Returning [-1, -1]\")\n",
    "            return [-1, -1]\n",
    "    except Exception as e:\n",
    "        eval_logger.debug(f\"Error: {e}. Returning [-1, -1]\")\n",
    "        return [-1, -1]\n",
    "\n",
    "def llava_process_results(doc, result):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        doc: a instance of the eval dataset\n",
    "        results: [pred]\n",
    "    Returns:\n",
    "        a dictionary with key: metric name (in this case coco_bleu), value: metric value\n",
    "    \"\"\"\n",
    "    try:\n",
    "        question = doc.get(\"question\", \"\")\n",
    "        ans1 = doc.get(\"gpt_answer\", \"\")\n",
    "        ans2 = result[0] if result else \"\"\n",
    "        captions = doc.get(\"caption\", [])\n",
    "        context = \"\\n\".join(captions) if isinstance(captions, list) else captions\n",
    "        category = \"llava_bench_\" + doc.get(\"category\", \"\")\n",
    "        rule = rule_dict.get(category, {})\n",
    "        prompt = rule.get(\"prompt\", \"\")\n",
    "        role = rule.get(\"role\", \"user\")\n",
    "        content = f\"[Context]\\n{context}\\n\\n\" f\"[Question]\\n{question}\\n\\n\" f\"[{role} 1]\\n{ans1}\\n\\n[End of {role} 1]\\n\\n\" f\"[{role} 2]\\n{ans2}\\n\\n[End of {role} 2]\\n\\n\" f\"[System]\\n{prompt}\\n\\n\"\n",
    "\n",
    "        review, model_name = get_eval(content, 1024)\n",
    "        scores = parse_score(review)\n",
    "    except Exception as e:\n",
    "        eval_logger.error(f\"Error for Question ID: {doc.get('question_id', 'Unknown')}: {e}\")\n",
    "        review = \"Failed to Get a Proper Review.\"\n",
    "        model_name = \"Failed Request\"\n",
    "        scores = [-1, -1]\n",
    "\n",
    "    metric = f\"gpt_eval_llava_{doc.get('category', 'all')}\"\n",
    "    category_review_dict = {\"question\": question, \"ans1\": ans1, \"ans2\": ans2, \"context\": context, \"category\": category, \"review\": review, \"scores\": scores, \"eval_model\": model_name, \"content\": content}\n",
    "\n",
    "    non_category_review_dict = deepcopy(category_review_dict)\n",
    "    non_category_review_dict[\"scores\"] = [-999, -999]\n",
    "    \n",
    "    data_dict = {}\n",
    "    for m in LLAVA_W_METRICS:\n",
    "        if m == metric:\n",
    "            data_dict[m] = category_review_dict\n",
    "        else:\n",
    "            data_dict[m] = non_category_review_dict\n",
    "    data_dict[\"gpt_eval_llava_all\"] = category_review_dict\n",
    "\n",
    "    # return {\"gpt_eval_llava_all\": review_dict}\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregation functions: averages all the scores from the results to produce a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llava_conv_aggregation(results):\n",
    "    return llava_aggregation(results, \"conv\")\n",
    "\n",
    "\n",
    "def llava_complex_aggregation(results):\n",
    "    return llava_aggregation(results, \"complex\")\n",
    "\n",
    "\n",
    "def llava_detail_aggregation(results):\n",
    "    return llava_aggregation(results, \"detail\")\n",
    "\n",
    "\n",
    "def llava_all_aggregation(results):\n",
    "    return llava_aggregation(results, \"all\")\n",
    "\n",
    "\n",
    "def llava_aggregation(results, category):\n",
    "    try:\n",
    "        scores = []\n",
    "        for result in results:\n",
    "            if -999 in result[\"scores\"]:\n",
    "                continue\n",
    "            scores.append(result[\"scores\"])\n",
    "\n",
    "        stats = np.asarray(scores).mean(0).tolist()\n",
    "        stats = [round(x, 3) for x in stats]\n",
    "        # gpt4_score_percentage = stats[0] * 10\n",
    "        # model_score_percentage = stats[1] * 10\n",
    "        # eval_logger.info(f\"Category: {category}\")\n",
    "        # eval_logger.info(f\"GPT4 Score: {gpt4_score_percentage:.1f}%\")\n",
    "        # eval_logger.info(f\"Model Score: {model_score_percentage:.1f}%\")\n",
    "        # eval_logger.info(\"=========================\")\n",
    "        return round(stats[1] / stats[0] * 100, 1)\n",
    "    except Exception as e:\n",
    "        eval_logger.info(f\"Error in llava_aggregation: {e}, and in category: {category}\")\n",
    "        return None\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
