{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import openai\n",
    "import requests\n",
    "import yaml\n",
    "from loguru import logger as eval_logger\n",
    "from openai import OpenAI\n",
    "\n",
    "NUM_SECONDS_TO_SLEEP = 5 # This defines the number of seconds to wait between retries when attempting to contact the OpenAI server.\n",
    "\n",
    "MULTIPLE_CHOICE_PROMPT = '\\nAnswer with the option\\'s letter from the given choices directly.'\n",
    "OPEN_ENDED_PROMPT = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Unable to find '/workdir/important_datasets/AGRIVQA_light/500P/dev-00000-of-00001.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      4\u001b[0m data_files \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/workdir/important_datasets/AGRIVQA_light/500P/dev-00000-of-00001.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/workdir/important_datasets/AGRIVQA_light/500P/test-00000-of-00001.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/workdir/important_datasets/AGRIVQA_light/500P/validation-00000-of-00001.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m      9\u001b[0m split \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/datasets/load.py:2523\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2518\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2519\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2520\u001b[0m )\n\u001b[1;32m   2522\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2523\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2536\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2537\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2538\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2540\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/datasets/load.py:2195\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   2193\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   2194\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[0;32m-> 2195\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2205\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2206\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2207\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/datasets/load.py:1736\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1713\u001b[0m \u001b[38;5;66;03m# We have several ways to get a dataset builder:\u001b[39;00m\n\u001b[1;32m   1714\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;66;03m# - if path is the name of a packaged dataset module\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1727\u001b[0m \n\u001b[1;32m   1728\u001b[0m \u001b[38;5;66;03m# Try packaged\u001b[39;00m\n\u001b[1;32m   1729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES:\n\u001b[1;32m   1730\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPackagedDatasetModuleFactory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1731\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1732\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1733\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1734\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1735\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[38;5;66;03m# Try locally\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mendswith(filename):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/datasets/load.py:1120\u001b[0m, in \u001b[0;36mPackagedDatasetModuleFactory.get_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1118\u001b[0m base_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mexpanduser()\u001b[38;5;241m.\u001b[39mresolve()\u001b[38;5;241m.\u001b[39mas_posix()\n\u001b[1;32m   1119\u001b[0m patterns \u001b[38;5;241m=\u001b[39m sanitize_patterns(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m get_data_patterns(base_path)\n\u001b[0;32m-> 1120\u001b[0m data_files \u001b[38;5;241m=\u001b[39m \u001b[43mDataFilesDict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1125\u001b[0m supports_metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01min\u001b[39;00m _MODULE_SUPPORTS_METADATA\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m supports_metadata \u001b[38;5;129;01mand\u001b[39;00m patterns \u001b[38;5;241m!=\u001b[39m DEFAULT_PATTERNS_ALL:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/datasets/data_files.py:689\u001b[0m, in \u001b[0;36mDataFilesDict.from_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    686\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m()\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, patterns_for_key \u001b[38;5;129;01min\u001b[39;00m patterns\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    688\u001b[0m     out[key] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 689\u001b[0m         \u001b[43mDataFilesList\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatterns_for_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    695\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(patterns_for_key, DataFilesList)\n\u001b[1;32m    696\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m patterns_for_key\n\u001b[1;32m    697\u001b[0m     )\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/datasets/data_files.py:594\u001b[0m, in \u001b[0;36mDataFilesList.from_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m patterns:\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    593\u001b[0m         data_files\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m--> 594\u001b[0m             \u001b[43mresolve_pattern\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m                \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    600\u001b[0m         )\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_magic(pattern):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/datasets/data_files.py:383\u001b[0m, in \u001b[0;36mresolve_pattern\u001b[0;34m(pattern, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m allowed_extensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    382\u001b[0m         error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m with any supported extension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(allowed_extensions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(error_msg)\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Unable to find '/workdir/important_datasets/AGRIVQA_light/500P/dev-00000-of-00001.parquet'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"parquet\"\n",
    "data_files = {\n",
    "    \"dev\": \"/workdir/important_datasets/AGRIVQA_light/500P/dev-00000-of-00001.parquet\",\n",
    "    \"test\": \"/workdir/important_datasets/AGRIVQA_light/500P/test-00000-of-00001.parquet\",\n",
    "    \"validation\": \"/workdir/important_datasets/AGRIVQA_light/500P/validation-00000-of-00001.parquet\"\n",
    "}\n",
    "split = \"dev\"\n",
    "\n",
    "dataset = load_dataset(dataset_name,data_files=data_files, split=split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, the .yaml file is read and converted into a dictionary named config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "__file__= \"/workdir/lmms-eval/lmms_eval/tasks/agrivqa_500P/agrivqa_500P.yaml\"\n",
    "\n",
    "rule_dict = json.load(open(os.path.join(os.path.dirname(os.path.abspath(__file__)), \"rule.json\"), \"r\"))\n",
    "\n",
    "with open(Path(__file__).parent / \"agrivqa_500P.yaml\", \"r\") as f:\n",
    "    raw_data = f.readlines()\n",
    "    safe_data = []\n",
    "    for i, line in enumerate(raw_data):\n",
    "        # remove function definition since yaml load cannot handle it\n",
    "        if \"!function\" not in line:\n",
    "            safe_data.append(line)\n",
    "\n",
    "    config = yaml.safe_load(\"\".join(safe_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, the variables for GPT-Eval are defined and assigned: MODEL_NAME, API_URL, and API_KEY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_EVAL_MODEL_NAME = \"gpt-4o-mini\"#config[\"metadata\"][\"gpt_eval_model_name\"]\n",
    "\n",
    "API_TYPE = 'openai' #os.getenv(\"API_TYPE\", \"openai\")\n",
    "\n",
    "if API_TYPE == \"openai\":\n",
    "    API_URL = os.getenv(\"OPENAI_API_URL\", \"https://api.openai.com/v1/chat/completions\")\n",
    "    API_KEY = os.getenv(\"OPENAI_API_KEY\", \"YOUR_API_KEY\")\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "elif API_TYPE == \"azure\":\n",
    "    API_URL = os.getenv(\"AZURE_ENDPOINT\", \"https://api.cognitive.microsoft.com/sts/v1.0/issueToken\")\n",
    "    API_KEY = os.getenv(\"AZURE_API_KEY\", \"YOUR_API_KEY\")\n",
    "    headers = {\n",
    "        \"api-key\": API_KEY,\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, as with all other tasks, the visuals and text are prepared for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llava_doc_to_visual(doc):\n",
    "    return [doc[\"image\"].convert(\"RGB\")]\n",
    "\n",
    "\n",
    "def agrivqa_500P_doc_to_text(doc, lmms_eval_specific_kwargs=None):\n",
    "    book_title = eval(doc['metadata']).get('book_title','')\n",
    "    chapter_title = eval(doc['metadata']).get('chapter_title','')\n",
    "    question = doc['question']\n",
    "    if lmms_eval_specific_kwargs is None:\n",
    "        lmms_eval_specific_kwargs = {}\n",
    "    pre_prompt = lmms_eval_specific_kwargs.get(\"pre_prompt\", \"\")\n",
    "    post_prompt = lmms_eval_specific_kwargs.get(\"post_prompt\", \"\")\n",
    "    context_prompt = lmms_eval_specific_kwargs.get(\"context_prompt\", \"\").format(book_title=book_title, chapter_title=chapter_title)\n",
    "    return f\"{pre_prompt}\\n\\nContext: {context_prompt}\\n\\nQuestion: {question}{post_prompt}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer to the following question.\n",
      "\n",
      "Context: This question is sourced from the book titled Beef Cattle, specifically found in the chapter Reproduction and Fattening\n",
      "\n",
      "Question: Is there a need for special care in the final phase of gestation and parturition of heifers?\n"
     ]
    }
   ],
   "source": [
    "doc = dataset[0]\n",
    "print(agrivqa_500P_doc_to_text(doc,config['lmms_eval_specific_kwargs']['default']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_eval() is the function responsible for contacting the OpenAI server and returning the evaluation.\n",
    "#### parse_score() extracts the response and returns the scores for the two answers: gpt_answer_score and llava_answer_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval(content: str, max_tokens: int, retries: int = 5):\n",
    "    global headers\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful and precise agronomy assistant for checking the quality of the answer.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": content},\n",
    "    ]\n",
    "\n",
    "    payload = {\n",
    "        \"model\": GPT_EVAL_MODEL_NAME,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.2,\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "\n",
    "    if API_TYPE == \"azure\":\n",
    "        payload.pop(\"model\")\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.post(API_URL, headers=headers, json=payload, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            response_data = response.json()\n",
    "\n",
    "            content = response_data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "            if content != \"\":\n",
    "                return content, response_data[\"model\"]\n",
    "            break  # If successful, break out of the loop\n",
    "\n",
    "        except Exception as e:\n",
    "            eval_logger.info(f\"Attempt {attempt + 1} failed with error: {e}\")\n",
    "            if attempt < retries:  # If we have retries left, sleep and then continue to next attempt\n",
    "                time.sleep(NUM_SECONDS_TO_SLEEP)\n",
    "            else:  # If this was the last attempt, log and return empty\n",
    "                eval_logger.error(f\"All {retries} attempts failed. Last error message: {e}\")\n",
    "                return \"\", \"\"\n",
    "    return \"\", \"\"\n",
    "\n",
    "\n",
    "def parse_score(review):\n",
    "    score = review.split(\"\\n\")[0]\n",
    "    score = score.replace(\",\", \" \")\n",
    "    try:\n",
    "        return float(score)\n",
    "    except ValueError:\n",
    "        eval_logger.debug(f\"Score not parsed: {review}. Returning -1\")\n",
    "        return -1\n",
    "\n",
    "def agrivqa_500P_process_results(doc, result):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        doc: a instance of the eval dataset\n",
    "        results: [pred]\n",
    "    Returns:\n",
    "        a dictionary with key: metric name (in this case coco_bleu), value: metric value\n",
    "    \"\"\"\n",
    "    try:\n",
    "        book_title = eval(doc['metadata']).get('book_title','')\n",
    "        chapter_title = eval(doc['metadata']).get('chapter_title','')\n",
    "        \n",
    "        question = doc.get(\"question\", \"\")\n",
    "        ans1 = doc.get(\"answer\", \"\")\n",
    "        role1= \"Expert\"\n",
    "        ans2 = result[0] if result else \"\"\n",
    "        role2 = rule_dict.get(\"role\", \"user\")\n",
    "        \n",
    "        captions = doc.get(\"caption\", [])\n",
    "        # TODO add the context label to the dataset (docs)\n",
    "        context = config['lmms_eval_specific_kwargs']['default']['context_prompt'].format(book_title=book_title, chapter_title=chapter_title)\n",
    "        prompt = rule_dict.get(\"prompt\", \"\")\n",
    "        content = f\"[Context]\\n{context}\\n\\n\" f\"[Question]\\n{question}\\n\\n\" f\"[{role1}]\\n{ans1}\\n\\n[End of {role1}]\\n\\n\" f\"[{role2}]\\n{ans2}\\n\\n[End of {role2}]\\n\\n\" f\"[System]\\n{prompt}\\n\\n\"\n",
    "        review, model_name = get_eval(content, 1024)\n",
    "        score = parse_score(review)\n",
    "    except Exception as e:\n",
    "        eval_logger.error(f\"Error for Question ID: {doc.get('question_id', 'Unknown')}: {e}\")\n",
    "        review = \"Failed to Get a Proper Review.\"\n",
    "        model_name = \"Failed Request\"\n",
    "        score = -1\n",
    "\n",
    "    review_dict = {'gpt_eval_agrivqa_500P': {\"question\": question, \"ans1\": ans1, \"ans2\": ans2, \"difficulty\": doc['topic_difficulty'], \"review\": review, \"score\": score, \"eval_model\": model_name, \"content\": content} }\n",
    "\n",
    "    return review_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=['''Yes, special care is indeed necessary during the final phase of gestation and parturition of heifers. This period is critical for both the health of the heifer and her calf. Specific considerations include: \n",
    "\n",
    "    Nutritional Needs : Ensuring adequate nutrition is vital to support the heifer's energy requirements and fetal development. This involves providing a balanced diet that includes sufficient proteins, vitamins, and minerals. \n",
    "\n",
    "    Monitoring Health : Regular health checks are important to identify any potential issues early, such as signs of distress or complications. \n",
    "\n",
    "    Environment : Providing a clean, comfortable, and stress-free environment is essential to help reduce the risk of infections and stress-related complications during parturition. \n",
    "\n",
    "    Assistance During Calving : Heifers may require assistance during calving, especially if they are experiencing their first parturition. Being prepared to intervene or call a veterinarian if necessary can help prevent complications. \n",
    "\n",
    "    Postpartum Care : After delivery, monitoring the heifer and her calf for any signs of complications, such as retained placenta or difficulty nursing, is crucial for their well-being. \n",
    "     \n",
    "Overall, attentive care during this period can help ensure a successful and safe calving process, promoting the health and productivity of both the heifer and her calf. ''']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agrivqa_500P_process_results(doc, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Context]\n",
      "This question is sourced from the book titled Beef Cattle, specifically found in the chapter Reproduction and Fattening\n",
      "\n",
      "[Question]\n",
      "Is there a need for special care in the final phase of gestation and parturition of heifers?\n",
      "\n",
      "[Expert]\n",
      "The breeder should be mainly concerned with the nutritional status of the breeding herd, especially in Central Brazil, since the final third of gestation coincides with the dry period. Animals that suffer from food restriction during this period give birth in poor physical conditions and present a long interval from parturition to the manifestation of the first estrus.\n",
      "\n",
      "In first-calving heifers, the problem is more severe, as they not only have to nurse their calves but are also in a growth phase, presenting high nutritional requirements during the lactation phase. At the time of parturition, it is recommended to use maternity pasture,\n",
      "\n",
      "[End of Expert]\n",
      "\n",
      "[Assistant]\n",
      "Yes, special care is indeed necessary during the final phase of gestation and parturition of heifers. This period is critical for both the health of the heifer and her calf. Specific considerations include: \n",
      "\n",
      "    Nutritional Needs : Ensuring adequate nutrition is vital to support the heifer's energy requirements and fetal development. This involves providing a balanced diet that includes sufficient proteins, vitamins, and minerals. \n",
      "\n",
      "    Monitoring Health : Regular health checks are important to identify any potential issues early, such as signs of distress or complications. \n",
      "\n",
      "    Environment : Providing a clean, comfortable, and stress-free environment is essential to help reduce the risk of infections and stress-related complications during parturition. \n",
      "\n",
      "    Assistance During Calving : Heifers may require assistance during calving, especially if they are experiencing their first parturition. Being prepared to intervene or call a veterinarian if necessary can help prevent complications. \n",
      "\n",
      "    Postpartum Care : After delivery, monitoring the heifer and her calf for any signs of complications, such as retained placenta or difficulty nursing, is crucial for their well-being. \n",
      "     \n",
      "Overall, attentive care during this period can help ensure a successful and safe calving process, promoting the health and productivity of both the heifer and her calf. \n",
      "\n",
      "[End of Assistant]\n",
      "\n",
      "[System]\n",
      "We would like to request your feedback on the performance of an AI assistant in response to the user question displayed above, which pertains to agronomic knowledge. Please compare the assistant's response to a provided expert response to assess alignment and accuracy.\n",
      "Rate the assistant's performance based on the following criteria:\n",
      "1. Accuracy in replicating the information provided in the expert response regarding agronomic practices and principles.\n",
      "2. Relevance to the specific agronomic context or query posed by the user.\n",
      "3. Assess the assistant's ability to identify key agronomic challenges, address specific crop needs, and predict outcomes effectively.\n",
      "4. Level of detail and clarity in the explanation of agronomic concepts.\n",
      "Provide an overall score on a scale of 1 to 10, where a higher score indicates better alignment with the expert response and overall performance.\n",
      "Please first output a single line containing only the numerical score. In the subsequent line, provide a comprehensive explanation of your evaluation, ensuring objectivity and clarity in your judgment, and specify any notable divergences or alignments observed in comparison to the expert response.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response['gpt_eval_agrivqa_500P']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregation functions: averages all the scores from the results to produce a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def agrivqa_500P_aggregation(results):\n",
    "    try:\n",
    "        scores = []\n",
    "        for result in results:\n",
    "            if result[\"score\"] == -1:\n",
    "                continue\n",
    "            scores.append(result[\"score\"])\n",
    "\n",
    "        stats = np.asarray(scores).mean(0).tolist()\n",
    "        stats = round(stats, 3)\n",
    "        return stats*10\n",
    "    except Exception as e:\n",
    "        eval_logger.info(f\"Error in agrivqa_500P_aggregation: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example run\n",
    "# python3 -m accelerate.commands.launch \\\n",
    "#     --num_processes=8 \\\n",
    "#     -m lmms_eval \\\n",
    "#     --model llava \\\n",
    "#     --model_args pretrained=\"liuhaotian/llava-v1.5-7b\" \\\n",
    "#     --tasks mme \\\n",
    "#     --batch_size 1 \\\n",
    "#     --log_samples \\\n",
    "#     --log_samples_suffix llava_v1.5_mme \\\n",
    "#     --output_path ./logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
