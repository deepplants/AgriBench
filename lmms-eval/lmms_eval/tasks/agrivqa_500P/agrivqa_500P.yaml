dataset_path: parquet
dataset_kwargs:
  data_files:
    dev: "/workdir/important_datasets/AGRIVQA_light/500P/dev-00000-of-00001.parquet"
    test: "/workdir/important_datasets/AGRIVQA_light/500P/test-00000-of-00001.parquet"
    validation: "/workdir/important_datasets/AGRIVQA_light/500P/validation-00000-of-00001.parquet"
task: "agrivqa_500P"
test_split: dev
output_type: generate_until
doc_to_visual: !function utils.llava_doc_to_visual
doc_to_text: !function utils.agrivqa_500P_doc_to_text
doc_to_target: "answer"

generation_kwargs:
  max_new_tokens: 4096
  temperature: 0
  top_p: 1.0
  num_beams: 1
  do_sample: false
process_results: !function utils.agrivqa_500P_process_results
metric_list:
  - metric: gpt_eval_agrivqa_500P
    aggregation: !function utils.agrivqa_500P_aggregation
    higher_is_better: true
metadata:
  version: 0.0
  gpt_eval_model_name: "gpt-4o-mini" #"gpt-4-0613"
lmms_eval_specific_kwargs:
  default:
    context_prompt: "This question is sourced from the book titled {book_title}, specifically found in the chapter {chapter_title}"
    pre_prompt: "Answer to the following question."
    post_prompt: ""
