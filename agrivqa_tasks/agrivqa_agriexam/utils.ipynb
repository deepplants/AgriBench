{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import openai\n",
    "import requests\n",
    "import yaml\n",
    "from loguru import logger as eval_logger\n",
    "from openai import OpenAI\n",
    "\n",
    "MULTI_CHOICE_PROMPT = \"Answer with the option's letter from the given choices directly.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating dev split: 16 examples [00:00, 2556.63 examples/s]\n",
      "Generating test split: 624 examples [00:00, 82101.94 examples/s]\n",
      "Generating validation split: 5613 examples [00:00, 194953.86 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "task_name = 'AgriExam'\n",
    "\n",
    "dataset_name = \"parquet\"\n",
    "data_files = {\n",
    "    \"dev\": \"/workdir/important_datasets/AGRIVQA_light/\"+task_name+\"/dev-00000-of-00001.parquet\",\n",
    "    \"test\": \"/workdir/important_datasets/AGRIVQA_light/\"+task_name+\"/test-00000-of-00001.parquet\",\n",
    "    \"validation\": \"/workdir/important_datasets/AGRIVQA_light/\"+task_name+\"/validation-00000-of-00001.parquet\"\n",
    "}\n",
    "split = \"dev\"\n",
    "\n",
    "dataset = load_dataset(dataset_name,data_files=data_files, split=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'dev__agriexam_1',\n",
       " 'question': 'Japanese style of gardens also known as?',\n",
       " 'options': \"['Formal garden', 'English garden', 'Mughal garden', 'Informal garden']\",\n",
       " 'explanation': None,\n",
       " 'image_1': None,\n",
       " 'image_2': None,\n",
       " 'image_3': None,\n",
       " 'image_4': None,\n",
       " 'image_5': None,\n",
       " 'img_type': '',\n",
       " 'answer': 'D',\n",
       " 'topic_difficulty': '3',\n",
       " 'question_type': 'multiple-choice',\n",
       " 'subfield': '{subfield}',\n",
       " 'metadata': '{\"source\": \"AgriExam\", \"author\": \"AgriExam\", \"license\": \"\", \"url\": \"https://www.agriexam.com/rajasthan-pre-pg-old-question-paper-2018\", \"language\": \"English\", \"verbose_answer\": \"Informal garden\"}'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, the .yaml file is read and converted into a dictionary named config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "__file__= \"/workdir/lmms-eval/lmms_eval/tasks/agrivqa_500P/agrivqa_500P.yaml\"\n",
    "\n",
    "rule_dict = json.load(open(os.path.join(os.path.dirname(os.path.abspath(__file__)), \"rule.json\"), \"r\"))\n",
    "\n",
    "with open(Path(__file__).parent / \"agrivqa_500P.yaml\", \"r\") as f:\n",
    "    raw_data = f.readlines()\n",
    "    safe_data = []\n",
    "    for i, line in enumerate(raw_data):\n",
    "        # remove function definition since yaml load cannot handle it\n",
    "        if \"!function\" not in line:\n",
    "            safe_data.append(line)\n",
    "\n",
    "    config = yaml.safe_load(\"\".join(safe_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 3.06k/3.06k [00:00<00:00, 9.80MB/s]\n",
      "Downloading data: 100%|██████████| 136M/136M [00:03<00:00, 36.4MB/s] \n",
      "Downloading data: 100%|██████████| 134M/134M [00:03<00:00, 38.4MB/s] \n",
      "Generating validation split: 100%|██████████| 4241/4241 [00:00<00:00, 10968.84 examples/s]\n",
      "Generating test split: 100%|██████████| 4241/4241 [00:00<00:00, 13671.71 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"lmms-lab/ScienceQA\", \"ScienceQA-FULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "options=ds['test']['choices'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chiasmus'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_options(options):\n",
    "    option_letters = [chr(ord(\"A\") + i) for i in range(len(options))]\n",
    "    choices_str = \"\\n\".join([f\"{option_letter}. {option}\" for option_letter, option in zip(option_letters, options)])\n",
    "    return choices_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, as with all other tasks, the visuals and text are prepared for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llava_doc_to_visual(doc):\n",
    "    return [doc[\"image\"].convert(\"RGB\")]\n",
    "\n",
    "\n",
    "def agrivqa_500P_doc_to_text(doc, lmms_eval_specific_kwargs=None):\n",
    "    book_title = eval(doc['metadata']).get('book_title','')\n",
    "    chapter_title = eval(doc['metadata']).get('chapter_title','')\n",
    "    question = doc['question']\n",
    "    if lmms_eval_specific_kwargs is None:\n",
    "        lmms_eval_specific_kwargs = {}\n",
    "    pre_prompt = lmms_eval_specific_kwargs.get(\"pre_prompt\", \"\")\n",
    "    post_prompt = lmms_eval_specific_kwargs.get(\"post_prompt\", \"\")\n",
    "    context_prompt = lmms_eval_specific_kwargs.get(\"context_prompt\", \"\").format(book_title=book_title, chapter_title=chapter_title)\n",
    "    return f\"{pre_prompt}\\n\\nContext: {context_prompt}\\n\\nQuestion: {question}{post_prompt}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer to the following question.\n",
      "\n",
      "Context: This question is sourced from the book titled Beef Cattle, specifically found in the chapter Reproduction and Fattening\n",
      "\n",
      "Question: Is there a need for special care in the final phase of gestation and parturition of heifers?\n"
     ]
    }
   ],
   "source": [
    "doc = dataset[0]\n",
    "print(agrivqa_500P_doc_to_text(doc,config['lmms_eval_specific_kwargs']['default']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_eval() is the function responsible for contacting the OpenAI server and returning the evaluation.\n",
    "#### parse_score() extracts the response and returns the scores for the two answers: gpt_answer_score and llava_answer_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval(content: str, max_tokens: int, retries: int = 5):\n",
    "    global headers\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful and precise agronomy assistant for checking the quality of the answer.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": content},\n",
    "    ]\n",
    "\n",
    "    payload = {\n",
    "        \"model\": GPT_EVAL_MODEL_NAME,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.2,\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "\n",
    "    if API_TYPE == \"azure\":\n",
    "        payload.pop(\"model\")\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.post(API_URL, headers=headers, json=payload, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            response_data = response.json()\n",
    "\n",
    "            content = response_data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "            if content != \"\":\n",
    "                return content, response_data[\"model\"]\n",
    "            break  # If successful, break out of the loop\n",
    "\n",
    "        except Exception as e:\n",
    "            eval_logger.info(f\"Attempt {attempt + 1} failed with error: {e}\")\n",
    "            if attempt < retries:  # If we have retries left, sleep and then continue to next attempt\n",
    "                time.sleep(NUM_SECONDS_TO_SLEEP)\n",
    "            else:  # If this was the last attempt, log and return empty\n",
    "                eval_logger.error(f\"All {retries} attempts failed. Last error message: {e}\")\n",
    "                return \"\", \"\"\n",
    "    return \"\", \"\"\n",
    "\n",
    "\n",
    "def parse_score(review):\n",
    "    score = review.split(\"\\n\")[0]\n",
    "    score = score.replace(\",\", \" \")\n",
    "    try:\n",
    "        return float(score)\n",
    "    except ValueError:\n",
    "        eval_logger.debug(f\"Score not parsed: {review}. Returning -1\")\n",
    "        return -1\n",
    "\n",
    "def agrivqa_500P_process_results(doc, result):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        doc: a instance of the eval dataset\n",
    "        results: [pred]\n",
    "    Returns:\n",
    "        a dictionary with key: metric name (in this case coco_bleu), value: metric value\n",
    "    \"\"\"\n",
    "    try:\n",
    "        book_title = eval(doc['metadata']).get('book_title','')\n",
    "        chapter_title = eval(doc['metadata']).get('chapter_title','')\n",
    "        \n",
    "        question = doc.get(\"question\", \"\")\n",
    "        ans1 = doc.get(\"answer\", \"\")\n",
    "        role1= \"Expert\"\n",
    "        ans2 = result[0] if result else \"\"\n",
    "        role2 = rule_dict.get(\"role\", \"user\")\n",
    "        \n",
    "        captions = doc.get(\"caption\", [])\n",
    "        # TODO add the context label to the dataset (docs)\n",
    "        context = config['lmms_eval_specific_kwargs']['default']['context_prompt'].format(book_title=book_title, chapter_title=chapter_title)\n",
    "        prompt = rule_dict.get(\"prompt\", \"\")\n",
    "        content = f\"[Context]\\n{context}\\n\\n\" f\"[Question]\\n{question}\\n\\n\" f\"[{role1}]\\n{ans1}\\n\\n[End of {role1}]\\n\\n\" f\"[{role2}]\\n{ans2}\\n\\n[End of {role2}]\\n\\n\" f\"[System]\\n{prompt}\\n\\n\"\n",
    "        review, model_name = get_eval(content, 1024)\n",
    "        score = parse_score(review)\n",
    "    except Exception as e:\n",
    "        eval_logger.error(f\"Error for Question ID: {doc.get('question_id', 'Unknown')}: {e}\")\n",
    "        review = \"Failed to Get a Proper Review.\"\n",
    "        model_name = \"Failed Request\"\n",
    "        score = -1\n",
    "\n",
    "    review_dict = {'gpt_eval_agrivqa_500P': {\"question\": question, \"ans1\": ans1, \"ans2\": ans2, \"difficulty\": doc['topic_difficulty'], \"review\": review, \"score\": score, \"eval_model\": model_name, \"content\": content} }\n",
    "\n",
    "    return review_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=['''Yes, special care is indeed necessary during the final phase of gestation and parturition of heifers. This period is critical for both the health of the heifer and her calf. Specific considerations include: \n",
    "\n",
    "    Nutritional Needs : Ensuring adequate nutrition is vital to support the heifer's energy requirements and fetal development. This involves providing a balanced diet that includes sufficient proteins, vitamins, and minerals. \n",
    "\n",
    "    Monitoring Health : Regular health checks are important to identify any potential issues early, such as signs of distress or complications. \n",
    "\n",
    "    Environment : Providing a clean, comfortable, and stress-free environment is essential to help reduce the risk of infections and stress-related complications during parturition. \n",
    "\n",
    "    Assistance During Calving : Heifers may require assistance during calving, especially if they are experiencing their first parturition. Being prepared to intervene or call a veterinarian if necessary can help prevent complications. \n",
    "\n",
    "    Postpartum Care : After delivery, monitoring the heifer and her calf for any signs of complications, such as retained placenta or difficulty nursing, is crucial for their well-being. \n",
    "     \n",
    "Overall, attentive care during this period can help ensure a successful and safe calving process, promoting the health and productivity of both the heifer and her calf. ''']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agrivqa_500P_process_results(doc, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Context]\n",
      "This question is sourced from the book titled Beef Cattle, specifically found in the chapter Reproduction and Fattening\n",
      "\n",
      "[Question]\n",
      "Is there a need for special care in the final phase of gestation and parturition of heifers?\n",
      "\n",
      "[Expert]\n",
      "The breeder should be mainly concerned with the nutritional status of the breeding herd, especially in Central Brazil, since the final third of gestation coincides with the dry period. Animals that suffer from food restriction during this period give birth in poor physical conditions and present a long interval from parturition to the manifestation of the first estrus.\n",
      "\n",
      "In first-calving heifers, the problem is more severe, as they not only have to nurse their calves but are also in a growth phase, presenting high nutritional requirements during the lactation phase. At the time of parturition, it is recommended to use maternity pasture,\n",
      "\n",
      "[End of Expert]\n",
      "\n",
      "[Assistant]\n",
      "Yes, special care is indeed necessary during the final phase of gestation and parturition of heifers. This period is critical for both the health of the heifer and her calf. Specific considerations include: \n",
      "\n",
      "    Nutritional Needs : Ensuring adequate nutrition is vital to support the heifer's energy requirements and fetal development. This involves providing a balanced diet that includes sufficient proteins, vitamins, and minerals. \n",
      "\n",
      "    Monitoring Health : Regular health checks are important to identify any potential issues early, such as signs of distress or complications. \n",
      "\n",
      "    Environment : Providing a clean, comfortable, and stress-free environment is essential to help reduce the risk of infections and stress-related complications during parturition. \n",
      "\n",
      "    Assistance During Calving : Heifers may require assistance during calving, especially if they are experiencing their first parturition. Being prepared to intervene or call a veterinarian if necessary can help prevent complications. \n",
      "\n",
      "    Postpartum Care : After delivery, monitoring the heifer and her calf for any signs of complications, such as retained placenta or difficulty nursing, is crucial for their well-being. \n",
      "     \n",
      "Overall, attentive care during this period can help ensure a successful and safe calving process, promoting the health and productivity of both the heifer and her calf. \n",
      "\n",
      "[End of Assistant]\n",
      "\n",
      "[System]\n",
      "We would like to request your feedback on the performance of an AI assistant in response to the user question displayed above, which pertains to agronomic knowledge. Please compare the assistant's response to a provided expert response to assess alignment and accuracy.\n",
      "Rate the assistant's performance based on the following criteria:\n",
      "1. Accuracy in replicating the information provided in the expert response regarding agronomic practices and principles.\n",
      "2. Relevance to the specific agronomic context or query posed by the user.\n",
      "3. Assess the assistant's ability to identify key agronomic challenges, address specific crop needs, and predict outcomes effectively.\n",
      "4. Level of detail and clarity in the explanation of agronomic concepts.\n",
      "Provide an overall score on a scale of 1 to 10, where a higher score indicates better alignment with the expert response and overall performance.\n",
      "Please first output a single line containing only the numerical score. In the subsequent line, provide a comprehensive explanation of your evaluation, ensuring objectivity and clarity in your judgment, and specify any notable divergences or alignments observed in comparison to the expert response.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response['gpt_eval_agrivqa_500P']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregation functions: averages all the scores from the results to produce a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def agrivqa_500P_aggregation(results):\n",
    "    try:\n",
    "        scores = []\n",
    "        for result in results:\n",
    "            if result[\"score\"] == -1:\n",
    "                continue\n",
    "            scores.append(result[\"score\"])\n",
    "\n",
    "        stats = np.asarray(scores).mean(0).tolist()\n",
    "        stats = round(stats, 3)\n",
    "        return stats*10\n",
    "    except Exception as e:\n",
    "        eval_logger.info(f\"Error in agrivqa_500P_aggregation: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example run\n",
    "# python3 -m accelerate.commands.launch \\\n",
    "#     --num_processes=8 \\\n",
    "#     -m lmms_eval \\\n",
    "#     --model llava \\\n",
    "#     --model_args pretrained=\"liuhaotian/llava-v1.5-7b\" \\\n",
    "#     --tasks mme \\\n",
    "#     --batch_size 1 \\\n",
    "#     --log_samples \\\n",
    "#     --log_samples_suffix llava_v1.5_mme \\\n",
    "#     --output_path ./logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
