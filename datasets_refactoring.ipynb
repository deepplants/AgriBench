{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Features, Value, Sequence, Image, DatasetInfo, Dataset, DatasetDict\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import json\n",
    "import re\n",
    "from copy import deepcopy\n",
    "\n",
    "import asyncio\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from ollama import AsyncClient\n",
    "\n",
    "from random import sample\n",
    "from collections import Counter\n",
    "from openai import OpenAI\n",
    "\n",
    "from workdir.utils import save_json, load_json\n",
    "\n",
    "from random import shuffle, sample\n",
    "from workdir.utils import load_dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_dataset_dict(folder_path, concat=False): # TODO check that work with multiple files\n",
    "#     dataset_name = \"parquet\"\n",
    "#     data_files = {\n",
    "#         \"dev\": f\"{folder_path}/dev-0*.parquet\",\n",
    "#         \"test\": f\"{folder_path}/test-*.parquet\",\n",
    "#         \"validation\": f\"{folder_path}/validation-*.parquet\"\n",
    "#     }\n",
    "#     dataset = load_dataset(dataset_name,data_files=data_files)\n",
    "#     if concat:\n",
    "#         return concatenate_datasets([dataset['dev'], dataset['test'], dataset['validation']])\n",
    "#     return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and Save utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 420\n",
    "\n",
    "           \n",
    "def split_dataset(dataset, dev_count=16, test_ratio=0.8, validation_ratio=None, random_seed=RANDOM_SEED):\n",
    "    \"\"\"\n",
    "    Splits a Hugging Face dataset into dev, test, and validation splits.\n",
    "    If only one of test_ratio or validation_ratio is provided, the other is inferred.\n",
    "    \n",
    "    Args:\n",
    "        dataset (Dataset): Hugging Face Dataset to split.\n",
    "        dev_count (int): Number of rows for the dev set.\n",
    "        test_ratio (float, optional): Proportion of remaining data for the test set.\n",
    "        validation_ratio (float, optional): Proportion of remaining data for the validation set.\n",
    "                                           test_ratio + validation_ratio must equal 1.\n",
    "    \n",
    "    Returns:\n",
    "        DatasetDict: Dictionary containing 'dev', 'test', and 'validation' splits.\n",
    "    \"\"\"\n",
    "    # Handle missing ratios\n",
    "    if test_ratio is None and validation_ratio is None:\n",
    "        raise ValueError(\"At least one of test_ratio or validation_ratio must be provided.\")\n",
    "    if test_ratio is None:\n",
    "        test_ratio = 1 - validation_ratio\n",
    "    if validation_ratio is None:\n",
    "        validation_ratio = 1 - test_ratio\n",
    "\n",
    "    # Validate ratios\n",
    "    if not (0 <= test_ratio <= 1 and 0 <= validation_ratio <= 1):\n",
    "        raise ValueError(\"Ratios must be between 0 and 1.\")\n",
    "    if round(test_ratio + validation_ratio, 6) != 1:\n",
    "        raise ValueError(\"test_ratio and validation_ratio must add up to 1.\")\n",
    "\n",
    "    # Shuffle the dataset for randomness\n",
    "    dataset = dataset.shuffle(seed=random_seed)\n",
    "    total_rows = len(dataset)\n",
    "    \n",
    "    # Step 1: Extract 'dev_count' rows for the dev set\n",
    "    if dev_count >= total_rows:\n",
    "        raise ValueError(\"dev_count is too large for the dataset size.\")\n",
    "    \n",
    "    dev_set = dataset.select(range(dev_count))\n",
    "    remaining_dataset = dataset.select(range(dev_count, total_rows))\n",
    "\n",
    "    # Step 2: Use train_test_split to split remaining data into test and validation\n",
    "    split = remaining_dataset.train_test_split(test_size=validation_ratio, seed=random_seed)\n",
    "    \n",
    "    # Step 3: Combine splits into DatasetDict\n",
    "    return DatasetDict({\n",
    "        \"dev\": dev_set,\n",
    "        \"test\": split['train'],        # 1 - validation_ratio\n",
    "        \"validation\": split['test']    # validation_ratio\n",
    "    })\n",
    "    \n",
    "    \n",
    "            \n",
    "def rename_split(split_dict, dataset_name=None):\n",
    "    # split_dict is a dictionary {split_name: dataset}\n",
    "    # renames row['id'] adding split_name and increasing id\n",
    "    \n",
    "    renamed_split_dict = {}\n",
    "    \n",
    "    def rename_row(row, index, split_name, dataset_name=None):\n",
    "        if not dataset_name:\n",
    "            dataset_name = row['id']\n",
    "        row['id'] = f\"{split_name}_{dataset_name}_{index+1}\"\n",
    "        return row\n",
    "    \n",
    "    for split, dataset in split_dict.items():\n",
    "        renamed_split_dict[split] = dataset.map(lambda row, index: rename_row(row, index, split_name=split, dataset_name=dataset_name), with_indices=True)\n",
    "    \n",
    "    return renamed_split_dict\n",
    "        \n",
    "        \n",
    "            \n",
    "def split_and_save(all_rows,\n",
    "                   root_folder = '/workdir/important_datasets/AGRIVQA/',\n",
    "                   max_len = None,\n",
    "                   dataset_name = None,\n",
    "                   dev_count = 16, test_ratio = 0.8, validation_ratio = None,\n",
    "                   random_seed = RANDOM_SEED):\n",
    "    \n",
    "    if not dataset_name:\n",
    "        dataset_name = all_rows[0]['id']\n",
    "    \n",
    "    dataset = Dataset.from_list(all_rows)\n",
    "    \n",
    "    split_dict = split_dataset(dataset, dev_count=dev_count, test_ratio=test_ratio, validation_ratio=validation_ratio, random_seed=random_seed)\n",
    "    \n",
    "    split_dict = rename_split(split_dict, dataset_name=dataset_name)\n",
    "    \n",
    "    for split, dataset in split_dict.items():\n",
    "        if not max_len:\n",
    "            chunk_size = len(dataset)\n",
    "        else:\n",
    "            chunk_size = max_len\n",
    "    \n",
    "        total_rows = len(dataset)\n",
    "        \n",
    "        num_files = 1 if total_rows == chunk_size else total_rows // chunk_size + 1\n",
    "        \n",
    "        for i in range(0, total_rows, chunk_size):\n",
    "        # Select a batch of rows from the dataset\n",
    "            chunk = dataset.select(range(i, min(i + chunk_size, total_rows)))\n",
    "            \n",
    "            # Create the file name with the appropriate split and chunk indices\n",
    "            file_name = f'{split}-{str(i // chunk_size).zfill(5)}-of-{str(num_files).zfill(5)}.parquet'\n",
    "            file_path = os.path.join(root_folder, dataset_name, file_name)\n",
    "            \n",
    "            # Make sure the directory exists\n",
    "            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "            \n",
    "            # Save the chunk to a Parquet file\n",
    "            print(f'Saving dataset file: {file_path}')\n",
    "            chunk.to_parquet(file_path)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AgriExam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AgriExam_row = {\n",
    "    \"id\": \"AgriExam\",\n",
    "    \"question\": \"{question}\",\n",
    "    \"options\": [\"{option_1}\", \"{option_2}\", \"{option_3}\", \"{option_4}\"],  # Assuming options are strings in a list\n",
    "    \"answer\": \"{answer}\",\n",
    "    \"category\": \"{category}\",  # Matches the 'category' field\n",
    "    \"question_type\": \"{question_type}\",  # Matches the 'question_type' field\n",
    "    \"metadata\":\n",
    "        {\n",
    "            \"source\": \"{source}\",\n",
    "            \"license\": \"{license}\",\n",
    "            \"url\": \"{url}\",\n",
    "            \"language\": \"{language}\",\n",
    "            \"verbose_answer\": \"{verbose_answer}\"\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:00<00:00, 19975.28it/s]\n",
      "100%|██████████| 192/192 [00:00<00:00, 20349.89it/s]\n",
      "  0%|          | 0/347 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 347/347 [00:00<00:00, 20167.16it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 20200.36it/s]\n",
      "100%|██████████| 317/317 [00:00<00:00, 19552.86it/s]\n",
      "100%|██████████| 745/745 [00:00<00:00, 20121.81it/s]\n",
      "100%|██████████| 155/155 [00:00<00:00, 19983.93it/s]\n",
      "100%|██████████| 114/114 [00:00<00:00, 20118.26it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 19750.29it/s]\n",
      "100%|██████████| 221/221 [00:00<00:00, 20311.62it/s]\n",
      "100%|██████████| 37/37 [00:00<00:00, 18554.43it/s]\n",
      "100%|██████████| 67/67 [00:00<00:00, 20510.79it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 20286.50it/s]\n",
      "100%|██████████| 99/99 [00:00<00:00, 19480.02it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 18406.16it/s]\n",
      "100%|██████████| 31/31 [00:00<00:00, 18819.43it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 18742.72it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 19686.02it/s]\n",
      "100%|██████████| 88/88 [00:00<00:00, 12275.88it/s]\n",
      "100%|██████████| 388/388 [00:00<00:00, 19717.10it/s]\n",
      "100%|██████████| 66/66 [00:00<00:00, 19889.64it/s]\n",
      "100%|██████████| 173/173 [00:00<00:00, 20069.55it/s]\n",
      "100%|██████████| 340/340 [00:00<00:00, 20085.68it/s]\n",
      "100%|██████████| 52/52 [00:00<00:00, 19405.98it/s]\n",
      "100%|██████████| 377/377 [00:00<00:00, 20078.12it/s]\n",
      "100%|██████████| 141/141 [00:00<00:00, 20153.93it/s]\n",
      "100%|██████████| 38/38 [00:00<00:00, 19638.19it/s]\n",
      "100%|██████████| 47/47 [00:00<00:00, 19196.83it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 18135.18it/s]\n"
     ]
    }
   ],
   "source": [
    "AgriExam_rows = []\n",
    "\n",
    "AgriExam_data = load_json('/workdir/agriexam_category_dictionary.json')\n",
    "\n",
    "for category, questions in AgriExam_data.items():\n",
    "\n",
    "    for q in tqdm(questions):\n",
    "        AgriExam_row['id'] = f\"AgriExam\"\n",
    "        AgriExam_row['question'] = q['question']\n",
    "        options = eval(q['options'])\n",
    "        if len(options) == 0:\n",
    "            options = []\n",
    "        AgriExam_row['options'] = options\n",
    "            \n",
    "        # Placeholder for explanation or any additional info\n",
    "        AgriExam_row['answer'] = q['answer']  # Assuming the correct answer is in q.correct_answer\n",
    "        \n",
    "        # Placeholder for category if applicable, adjust based on your requirements\n",
    "        AgriExam_row['category'] = category  # Replace with actual category logic if needed\n",
    "        \n",
    "        # Placeholder for question_type logic: assuming it's a multiple-choice or open-ended type\n",
    "        if options:\n",
    "            AgriExam_row['question_type'] = 'multiple-choice'\n",
    "        else:\n",
    "            AgriExam_row['question_type'] = 'open-ended'\n",
    "        \n",
    "        # Metadata section: Assuming placeholders for now\n",
    "        metadata = eval(q['metadata'])\n",
    "        \n",
    "        AgriExam_row['metadata']['source'] = metadata['source']  # Replace with actual source if needed\n",
    "        AgriExam_row['metadata']['license'] = metadata['license']\n",
    "        AgriExam_row['metadata']['url'] = metadata['url']\n",
    "        AgriExam_row['metadata']['language'] = metadata['language']\n",
    "        AgriExam_row['metadata']['verbose_answer'] = metadata['verbose_answer']\n",
    "\n",
    "        # Appending the row to rows\n",
    "        AgriExam_rows.append(deepcopy(AgriExam_row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save and check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 16/16 [00:00<00:00, 2103.00 examples/s]\n",
      "Map: 100%|██████████| 16160/16160 [00:02<00:00, 7731.92 examples/s] \n",
      "Map: 100%|██████████| 4041/4041 [00:00<00:00, 11189.11 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset file: /workdir/AGRIVQA/Agri500P/dev-00000-of-00001.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1058.37ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset file: /workdir/AGRIVQA/Agri500P/test-00000-of-00001.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 17/17 [00:00<00:00, 285.08ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset file: /workdir/AGRIVQA/Agri500P/validation-00000-of-00001.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 347.60ba/s]\n",
      "Generating dev split: 16 examples [00:00, 4116.10 examples/s]\n",
      "Generating test split: 16160 examples [00:00, 178046.87 examples/s]\n",
      "Generating validation split: 4041 examples [00:00, 152566.14 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'AgriExam'\n",
    "root_folder = '/workdir/AGRIVQA/'\n",
    "split_and_save(AgriExam_rows, root_folder=root_folder)\n",
    "\n",
    "dataset = load_dataset_dict(os.path.join(root_folder, dataset_name))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agri500P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agri500P_row = {\n",
    "    \"id\": \"Agri500P\",\n",
    "    \"question\": \"{question}\",\n",
    "    \"answer\": \"{answer}\",\n",
    "    \"context\": \"{context}\",\n",
    "    \"category\": \"{category}\",  # Example: Plant Science, Pests, Taxonomy\n",
    "    \"question_type\": \"{question_type}\",  # Example: multiple choice, open ended\n",
    "    \"metadata\":\n",
    "        {\n",
    "            \"source\": \"{source}\",\n",
    "            \"license\": \"{license}\",\n",
    "            \"language\": \"{language}\",\n",
    "            \"book_title\": \"{book_title}\",\n",
    "            \"chapter_title\": \"{chapter_title}\"\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = '500P'\n",
    "\n",
    "dataset_name = \"parquet\"\n",
    "data_files = {\n",
    "    \"dev\": \"/workdir/important_datasets/AGRIVQA/\"+task_name+\"/dev-00000-of-00001.parquet\",\n",
    "    \"test\": \"/workdir/important_datasets/AGRIVQA/\"+task_name+\"/test-00000-of-00001.parquet\",\n",
    "    \"validation\": \"/workdir/important_datasets/AGRIVQA/\"+task_name+\"/validation-00000-of-00001.parquet\"\n",
    "}\n",
    "\n",
    "dataset = load_dataset(dataset_name,data_files=data_files)\n",
    "\n",
    "Agri500P_dataset = concatenate_datasets([dataset['dev'], dataset['test'], dataset['validation']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20217/20217 [00:04<00:00, 4957.55it/s]\n"
     ]
    }
   ],
   "source": [
    "Agri500P_rows = []\n",
    "\n",
    "pt_en_title=load_json('/workdir/translate_title.json')\n",
    "en_pt_title = {en:pt for pt,en in pt_en_title.items()}\n",
    "\n",
    "Agri500P_category = load_json('/workdir/500P_categories.json')\n",
    "theme_category_dict = {theme:category  for category,theme_list in Agri500P_category.items() for theme in theme_list}\n",
    "\n",
    "\n",
    "for q in tqdm(Agri500P_dataset):\n",
    "    Agri500P_row['id'] = f\"Agri500P\"\n",
    "    Agri500P_row['question'] = q['question']\n",
    "        \n",
    "    # Placeholder for explanation or any additional info\n",
    "    Agri500P_row['answer'] = q['answer']  # Assuming the correct answer is in q.correct_answer\n",
    "    \n",
    "    # Metadata section: Assuming placeholders for now\n",
    "    metadata = eval(q['metadata'])\n",
    "    \n",
    "    # Placeholder for category if applicable, adjust based on your requirements\n",
    "    \n",
    "    book_title = metadata['book_title'].replace(' – The Producer Asks, Embrapa Answers', 's')\n",
    "    chapter_title = metadata['chapter_title'].replace('Strawberry production in greenhouses is a more common topic, but I assume you meant... ', '')\n",
    "    \n",
    "    category = theme_category_dict[f\"{book_title}: {chapter_title}\"]\n",
    "    \n",
    "    Agri500P_row['category'] = category  # Replace with actual category logic if needed\n",
    "    \n",
    "    Agri500P_row['context'] = f\"This question is sourced from the book titled '{book_title}', specifically found in the chapter '{chapter_title}'.\"\n",
    "    \n",
    "    # Placeholder for question_type logic: assuming it's a multiple-choice or open-ended type\n",
    "    if options:\n",
    "        Agri500P_row['question_type'] = 'multiple-choice'\n",
    "    else:\n",
    "        Agri500P_row['question_type'] = 'open-ended'\n",
    "    \n",
    "    \n",
    "    \n",
    "    Agri500P_row['metadata']['source'] = '500 Perguntas e 500 Respostas: ' + en_pt_title[book_title]  # Replace with actual source if needed\n",
    "    Agri500P_row['metadata']['license'] = metadata['license']\n",
    "    Agri500P_row['metadata']['language'] = metadata['language']\n",
    "    Agri500P_row['metadata']['book_title'] = metadata['book_title']\n",
    "    Agri500P_row['metadata']['chapter_title'] = metadata['chapter_title']\n",
    "    # Appending the row to rows\n",
    "    Agri500P_rows.append(deepcopy(Agri500P_row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save and check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 16/16 [00:00<00:00, 2395.89 examples/s]\n",
      "Map: 100%|██████████| 16160/16160 [00:01<00:00, 10164.16 examples/s]\n",
      "Map: 100%|██████████| 4041/4041 [00:00<00:00, 7711.41 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset file: /workdir/AGRIVQA/Agri500P/dev-00000-of-00001.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 842.91ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset file: /workdir/AGRIVQA/Agri500P/test-00000-of-00001.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 17/17 [00:00<00:00, 296.62ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset file: /workdir/AGRIVQA/Agri500P/validation-00000-of-00001.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 359.79ba/s]\n",
      "Generating dev split: 16 examples [00:00, 2684.68 examples/s]\n",
      "Generating test split: 16160 examples [00:00, 181283.62 examples/s]\n",
      "Generating validation split: 4041 examples [00:00, 215235.91 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    dev: Dataset({\n",
       "        features: ['id', 'question', 'answer', 'context', 'category', 'question_type', 'metadata'],\n",
       "        num_rows: 16\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'question', 'answer', 'context', 'category', 'question_type', 'metadata'],\n",
       "        num_rows: 16160\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'question', 'answer', 'context', 'category', 'question_type', 'metadata'],\n",
       "        num_rows: 4041\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = 'Agri500P'\n",
    "root_folder = '/workdir/AGRIVQA/'\n",
    "split_and_save(Agri500P_rows, root_folder=root_folder)\n",
    "\n",
    "dataset = load_dataset_dict(os.path.join(root_folder, dataset_name))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EPPO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'EPPO'\n",
    "\n",
    "dataset_name = \"parquet\"\n",
    "data_files = {\n",
    "    \"dev\": \"/workdir/important_datasets/AGRIVQA/\"+task_name+\"/dev-00000-of-00001.parquet\",\n",
    "    \"test\": \"/workdir/important_datasets/AGRIVQA/\"+task_name+\"/test-00000-of-00001.parquet\",\n",
    "    \"validation\": \"/workdir/important_datasets/AGRIVQA/\"+task_name+\"/validation-00000-of-00001.parquet\"\n",
    "}\n",
    "\n",
    "dataset = load_dataset(dataset_name,data_files=data_files)\n",
    "\n",
    "EPPO_dataset = concatenate_datasets([dataset['dev'], dataset['test'], dataset['validation']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_dict = {\n",
    "    'common_name':'Common Name',\n",
    "    'damage_cause': 'Taxonomy',\n",
    "    'genus_name': 'Taxonomy',\n",
    "    'growth_stage': 'Growth Stage',\n",
    "    'scientific_name': 'Taxonomy',\n",
    "    'weed_identification': 'Taxonomy'\n",
    "}\n",
    "\n",
    "kingdoms = [\n",
    " 'Archaea',\n",
    " 'Bacteria',\n",
    " 'Chromista',\n",
    " 'Viruses_and_viroids',\n",
    " 'Fungi',\n",
    " 'Protista',\n",
    " 'Plantae',\n",
    " 'Animalia']\n",
    "\n",
    "kingdom_eppo_dict = { k: list(load_json(f'/workdir/important_datasets/EPPO_to_GBIF/{k}_EPPO_to_GBIF.json')) for k in kingdoms}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPPO_row = {\n",
    "    \"id\": \"EPPO\",\n",
    "    \"question\": \"{question}\",\n",
    "    \"options\": [\"{option_1}\", \"{option_2}\", \"{option_3}\", \"{option_4}\"],  # List of options\n",
    "    \"answer\": \"{answer}\",\n",
    "    \"image\": \"{image_path}\",  # Path to the image\n",
    "    \"taxon_rank\": 0,  # Integer\n",
    "    \"options_difficulty\": 0,  # Integer\n",
    "    \"kingdom\": \"{kingdom}\",  # Example: Plantae, Animalia\n",
    "    \"category\": \"{category}\",  # Example: Taxonomy, Growth Stage, Common Name\n",
    "    \"question_template\": \"{question_template}\",  # Example: scientific_name, common_name\n",
    "    \"question_type\": \"{question_type}\",  # Example: multiple choice, open ended\n",
    "    \"metadata\":\n",
    "        {\n",
    "            \"source\": \"{source}\",\n",
    "            \"license\": \"{license}\",\n",
    "            \"image_url\": \"{image_url}\",\n",
    "            \"language\": \"{language}\",\n",
    "            \"verbose_answer\": \"{verbose_answer}\",\n",
    "            \"eppo_code\": \"{eppo_code}\",\n",
    "            \"gbif_key\": \"{gbif_key}\",\n",
    "            \"common_name_language\": \"{common_name_language}\"\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20648/20648 [03:30<00:00, 97.99it/s] \n"
     ]
    }
   ],
   "source": [
    "EPPO_rows = []\n",
    "\n",
    "OPTIONS = ['A','B','C','D','E']\n",
    "\n",
    "for q in tqdm(EPPO_dataset):\n",
    "    EPPO_row['id'] = f\"EPPO\"\n",
    "    EPPO_row['question'] = q['question']\n",
    "        \n",
    "    # Placeholder for explanation or any additional info\n",
    "    EPPO_row['answer'] = q['answer']  # Assuming the correct answer is in q.correct_answer\n",
    "    \n",
    "    options = eval(q['options'])\n",
    "    if len(options) == 0:\n",
    "            options = []\n",
    "    EPPO_row['options'] = options\n",
    "    \n",
    "    EPPO_row['image'] = q['image_1']\n",
    "    EPPO_row['options_difficulty'] = int(q['options_difficulty'])\n",
    "    # Metadata section: Assuming placeholders for now\n",
    "    metadata = eval(q['metadata'])\n",
    "    \n",
    "    kingdom = metadata['kingdom']\n",
    "    EPPO_row['kingdom'] = kingdom\n",
    "    EPPO_row['taxon_rank'] = kingdom_eppo_dict[kingdom].index(metadata['eppo_code'])\n",
    "    \n",
    "    # Placeholder for category if applicable, adjust based on your requirements\n",
    "    EPPO_row[\"question_template\"] = metadata['tag']\n",
    "    \n",
    "    EPPO_row['category'] = template_dict[metadata['tag']]  # Replace with actual category logic if needed\n",
    "    \n",
    "    \n",
    "    #EPPO_row['context'] = #f\"This question is sourced from the book titled '{book_title}', specifically found in the chapter '{chapter_title}'.\"\n",
    "    \n",
    "    # Placeholder for question_type logic: assuming it's a multiple-choice or open-ended type\n",
    "    if options:\n",
    "        EPPO_row['question_type'] = 'multiple-choice'\n",
    "    else:\n",
    "        EPPO_row['question_type'] = 'open-ended'\n",
    "    \n",
    "    \n",
    "    EPPO_row['metadata']['source'] = 'EPPO'  # Replace with actual source if needed\n",
    "    EPPO_row['metadata']['license'] = metadata['license']\n",
    "    EPPO_row['metadata']['language'] = metadata['language']\n",
    "    EPPO_row['metadata']['image_url'] = metadata['url']\n",
    "    EPPO_row['metadata']['verbose_answer'] = options[OPTIONS.index(q['answer'])]\n",
    "    EPPO_row['metadata']['eppo_code'] = metadata['eppo_code']\n",
    "    EPPO_row['metadata']['gbif_key'] = metadata['gbif_key']\n",
    "    EPPO_row['metadata']['common_name_language'] = metadata['common_name_language']\n",
    "    # Appending the row to rows\n",
    "    EPPO_rows.append(deepcopy(EPPO_row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save and check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 16/16 [00:00<00:00, 1360.57 examples/s]\n",
      "Map: 100%|██████████| 16505/16505 [00:12<00:00, 1371.13 examples/s]\n",
      "Map: 100%|██████████| 4127/4127 [00:02<00:00, 1872.95 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset file: /workdir/AGRIVQA/EPPO/dev-00000-of-00001.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 95.46ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset file: /workdir/AGRIVQA/EPPO/test-00000-of-00001.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 166/166 [00:04<00:00, 37.57ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset file: /workdir/AGRIVQA/EPPO/validation-00000-of-00001.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 42/42 [00:01<00:00, 35.87ba/s]\n",
      "Generating dev split: 16 examples [00:00, 664.75 examples/s]\n",
      "Generating test split: 16505 examples [00:04, 3662.74 examples/s]\n",
      "Generating validation split: 4127 examples [00:00, 4670.45 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    dev: Dataset({\n",
       "        features: ['id', 'question', 'options', 'answer', 'image', 'taxon_rank', 'options_difficulty', 'kingdom', 'category', 'question_template', 'question_type', 'metadata'],\n",
       "        num_rows: 16\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'question', 'options', 'answer', 'image', 'taxon_rank', 'options_difficulty', 'kingdom', 'category', 'question_template', 'question_type', 'metadata'],\n",
       "        num_rows: 16505\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'question', 'options', 'answer', 'image', 'taxon_rank', 'options_difficulty', 'kingdom', 'category', 'question_template', 'question_type', 'metadata'],\n",
       "        num_rows: 4127\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = 'EPPO'\n",
    "root_folder = '/workdir/AGRIVQA/'\n",
    "split_and_save(EPPO_rows, root_folder=root_folder)\n",
    "\n",
    "dataset = load_dataset_dict(os.path.join(root_folder, dataset_name))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBIF !!!TODO!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBIF_row = {\n",
    "    \"id\": \"_GBIF_{number}\",\n",
    "    \"question\": \"{question}\",\n",
    "    \"options\": [\"{option_1}\", \"{option_2}\", \"{option_3}\", \"{option_4}\"],  # List of options\n",
    "    \"answer\": \"{answer}\",\n",
    "    \"image_1\": \"{image_path_1}\",  # Path to image 1\n",
    "    \"image_2\": \"{image_path_2}\",  # Path to image 2\n",
    "    \"image_3\": \"{image_path_3}\",  # Path to image 3\n",
    "    \"image_4\": \"{image_path_4}\",  # Path to image 4\n",
    "    \"image_5\": \"{image_path_5}\",  # Path to image 5\n",
    "    \"options_difficulty\": 0,  # Integer\n",
    "    \"region\": \"{region}\",  # Example: Europe, Asia\n",
    "    \"event_date\": \"{event_date}\",  # Example: 2024-12-16\n",
    "    \"question_template\": \"{question_template}\",  # Example: common_name, scientific_name\n",
    "    \"question_type\": \"{question_type}\",  # Example: multiple choice, open ended\n",
    "    \"metadata\":\n",
    "        {\n",
    "            \"source\": \"{source}\",\n",
    "            \"license\": \"{license}\",\n",
    "            \"image_url\": [\"{image_url_1}\", \"{image_url_2}\"],  # List of image URLs\n",
    "            \"gbif_id\": \"{gbif_id}\",\n",
    "            \"eppo_codes\": [\"{eppo_code_1}\", \"{eppo_code_2}\"],  # List of EPPO codes\n",
    "            \"gbif_taxon_key\": \"{gbif_taxon_key}\",\n",
    "            \"language\": \"{language}\",\n",
    "            \"verbose_answer\": \"{verbose_answer}\"\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dev split: 16 examples [00:00, 191.98 examples/s]\n",
      "Generating test split: 9 examples [00:00, 158.35 examples/s]\n",
      "Generating validation split: 75 examples [00:00, 335.71 examples/s]\n"
     ]
    }
   ],
   "source": [
    "task_name = 'Identification'\n",
    "\n",
    "dataset_name = \"parquet\"\n",
    "data_files = {\n",
    "    \"dev\": \"/workdir/important_datasets/AGRIVQA/\"+task_name+\"/dev-00000-of-00001.parquet\",\n",
    "    \"test\": \"/workdir/important_datasets/AGRIVQA/\"+task_name+\"/test-00000-of-00001.parquet\",\n",
    "    \"validation\": \"/workdir/important_datasets/AGRIVQA/\"+task_name+\"/validation-00000-of-00001.parquet\"\n",
    "}\n",
    "\n",
    "dataset = load_dataset(dataset_name,data_files=data_files)\n",
    "\n",
    "EPPO_dataset = concatenate_datasets([dataset['dev'], dataset['test'], dataset['validation']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_dict = {\n",
    "    'common_name':'Common Name',\n",
    "    'damage_cause': 'Taxonomy',\n",
    "    'genus_name': 'Taxonomy',\n",
    "    'growth_stage': 'Growth Stage',\n",
    "    'scientific_name': 'Taxonomy',\n",
    "    'weed_identification': 'Taxonomy' \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kingdoms = [\n",
    " 'Archaea',\n",
    " 'Bacteria',\n",
    " 'Chromista',\n",
    " 'Viruses_and_viroids',\n",
    " 'Fungi',\n",
    " 'Protista',\n",
    " 'Plantae',\n",
    " 'Animalia']\n",
    "kingdom_eppo_dict = { k: list(load_json(f'/workdir/important_datasets/EPPO_to_GBIF/{k}_EPPO_to_GBIF.json')) for k in kingdoms}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'dev__EPPO_1',\n",
       " 'question': 'What is the common name in English of this plant? <image 1>',\n",
       " 'options': \"['Indian ginseng', 'violet tube flower', 'sticky tailflower', 'tomato']\",\n",
       " 'explanation': None,\n",
       " 'image_1': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x1365>,\n",
       " 'image_2': None,\n",
       " 'image_3': None,\n",
       " 'image_4': None,\n",
       " 'image_5': None,\n",
       " 'img_type': \"['Picture']\",\n",
       " 'answer': 'D',\n",
       " 'options_difficulty': '4',\n",
       " 'question_type': 'multiple-choice',\n",
       " 'subfield': 'Plantae / common_name',\n",
       " 'metadata': '{\"source\": \"EPPO\", \"author\": \"EPPO\", \"license\": \"\", \"url\": \"https://gd.eppo.int/media/data/taxon/L/LYPES/pics/1024x0/4984.jpg\", \"language\": \"English\", \"verbose_answer\": \"{verbose_answer}\", \"eppo_code\": \"LYPES\", \"gbif_key\": \"2930137\", \"kingdom\": \"Plantae\", \"tag\": \"common_name\", \"common_name_language\": \"English\"}'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPPO_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPPO_row = {\n",
    "    \"id\": \"EPPO\",\n",
    "    \"question\": \"{question}\",\n",
    "    \"options\": [\"{option_1}\", \"{option_2}\", \"{option_3}\", \"{option_4}\"],  # List of options\n",
    "    \"answer\": \"{answer}\",\n",
    "    \"image\": \"{image_path}\",  # Path to the image\n",
    "    \"taxon_rank\": 0,  # Integer\n",
    "    \"options_difficulty\": 0,  # Integer\n",
    "    \"kingdom\": \"{kingdom}\",  # Example: Plantae, Animalia\n",
    "    \"category\": \"{category}\",  # Example: Taxonomy, Growth Stage, Common Name\n",
    "    \"question_template\": \"{question_template}\",  # Example: scientific_name, common_name\n",
    "    \"question_type\": \"{question_type}\",  # Example: multiple choice, open ended\n",
    "    \"metadata\":\n",
    "        {\n",
    "            \"source\": \"{source}\",\n",
    "            \"license\": \"{license}\",\n",
    "            \"image_url\": \"{image_url}\",\n",
    "            \"language\": \"{language}\",\n",
    "            \"verbose_answer\": \"{verbose_answer}\",\n",
    "            \"eppo_code\": \"{eppo_code}\",\n",
    "            \"gbif_key\": \"{gbif_key}\",\n",
    "            \"common_name_language\": \"{common_name_language}\"\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20648/20648 [02:43<00:00, 126.65it/s]\n"
     ]
    }
   ],
   "source": [
    "EPPO_rows = []\n",
    "\n",
    "OPTIONS = ['A','B','C','D','E']\n",
    "\n",
    "for q in tqdm(EPPO_dataset):\n",
    "    EPPO_row['id'] = f\"EPPO\"\n",
    "    EPPO_row['question'] = q['question']\n",
    "        \n",
    "    # Placeholder for explanation or any additional info\n",
    "    EPPO_row['answer'] = q['answer']  # Assuming the correct answer is in q.correct_answer\n",
    "    \n",
    "    options = eval(q['options'])\n",
    "    if len(options) == 0:\n",
    "            options = []\n",
    "    EPPO_row['options'] = options\n",
    "    \n",
    "    EPPO_row['image'] = q['image_1']\n",
    "    EPPO_row['options_difficulty'] = int(q['options_difficulty'])\n",
    "    # Metadata section: Assuming placeholders for now\n",
    "    metadata = eval(q['metadata'])\n",
    "    \n",
    "    kingdom = metadata['kingdom']\n",
    "    EPPO_row['kingdom'] = kingdom\n",
    "    EPPO_row['taxon_rank'] = kingdom_eppo_dict[kingdom].index(metadata['eppo_code'])\n",
    "    \n",
    "    # Placeholder for category if applicable, adjust based on your requirements\n",
    "    EPPO_row[\"question_template\"] = metadata['tag']\n",
    "    \n",
    "    EPPO_row['category'] = template_dict[metadata['tag']]  # Replace with actual category logic if needed\n",
    "    \n",
    "    \n",
    "    #EPPO_row['context'] = #f\"This question is sourced from the book titled '{book_title}', specifically found in the chapter '{chapter_title}'.\"\n",
    "    \n",
    "    # Placeholder for question_type logic: assuming it's a multiple-choice or open-ended type\n",
    "    if options:\n",
    "        EPPO_row['question_type'] = 'multiple-choice'\n",
    "    else:\n",
    "        EPPO_row['question_type'] = 'open-ended'\n",
    "    \n",
    "    \n",
    "    EPPO_row['metadata']['source'] = 'EPPO'  # Replace with actual source if needed\n",
    "    EPPO_row['metadata']['license'] = metadata['license']\n",
    "    EPPO_row['metadata']['language'] = metadata['language']\n",
    "    EPPO_row['metadata']['image_url'] = metadata['url']\n",
    "    EPPO_row['metadata']['verbose_answer'] = options[OPTIONS.index(q['answer'])]\n",
    "    EPPO_row['metadata']['eppo_code'] = metadata['eppo_code']\n",
    "    EPPO_row['metadata']['gbif_key'] = metadata['gbif_key']\n",
    "    EPPO_row['metadata']['common_name_language'] = metadata['common_name_language']\n",
    "    # Appending the row to rows\n",
    "    EPPO_rows.append(deepcopy(EPPO_row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1824.40ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 567.18ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 529.65ba/s]\n",
      "Generating dev split: 16 examples [00:00, 4733.98 examples/s]\n",
      "Generating test split: 3625 examples [00:00, 321288.84 examples/s]\n",
      "Generating validation split: 907 examples [00:00, 206538.56 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'dev_AgriExam_1',\n",
       " 'question': 'Which of the following states is the largest producer of annual flowers’ seeds?',\n",
       " 'options': ['Punjab', 'Karnataka', 'Tamil Nadu', 'Kerala'],\n",
       " 'answer': 'A',\n",
       " 'category': 'Horticulture and Ornamental Plants',\n",
       " 'question_type': 'multiple-choice',\n",
       " 'metadata': {'language': 'English',\n",
       "  'license': '',\n",
       "  'source': 'AgriExam',\n",
       "  'url': 'https://www.agriexam.com/horticulture-jrf-2020',\n",
       "  'verbose_answer': 'Punjab'}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "split_and_save(EPPO_rows)\n",
    "path = '/workdir/important_datasets/AGRIVQA_v2'\n",
    "dataset_name = 'EPPO'\n",
    "\n",
    "dataset_dict = load_dataset('parquet', data_files={\n",
    "    'dev': f'{path}/{dataset_name}/dev-00000-of-00001.parquet',\n",
    "    'test': f'{path}/{dataset_name}/test-00000-of-00001.parquet',\n",
    "    'validation': f'{path}/{dataset_name}/validation-00000-of-00001.parquet'\n",
    "})\n",
    "\n",
    "dataset_dict['dev'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WikiHow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "WikiHow_row = {\n",
    "    \"id\": \"_WikiHow_{number}\",\n",
    "    \"question\": \"{question}\",\n",
    "    \"options\": [\"{option_1}\", \"{option_2}\", \"{option_3}\", \"{option_4}\"],  # List of options\n",
    "    \"answer\": \"{answer}\",\n",
    "    \"options_difficulty\": 0,  # Integer\n",
    "    \"category\": \"{category}\",  # Example: TODO\n",
    "    \"subcategory\": \"{subcategory}\",\n",
    "    \"question_type\": \"{question_type}\",  # Example: multiple choice, open ended\n",
    "    \"question_template\": \"{question_template}\",  # Example: questiontype\n",
    "    \"metadata\":\n",
    "        {\n",
    "            \"source\": \"{source}\",\n",
    "            \"license\": \"{license}\",\n",
    "            \"url\": \"{url}\",\n",
    "            \"language\": \"{language}\",\n",
    "            \"verbose_answer\": \"{verbose_answer}\"\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikihow_dataset = load_json('/workdir/wikihow/wikihow.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikihow_category = load_json('/workdir/wikihow/wikihow_category.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikihow_category_url = {url:category for category,urls in wikihow_category.items() for url in urls}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2141/2141 [00:00<00:00, 40791.32it/s]\n"
     ]
    }
   ],
   "source": [
    "WikiHow_rows = []\n",
    "\n",
    "OPTIONS = ['A','B','C','D','E']\n",
    "\n",
    "for q in tqdm(wikihow_dataset):\n",
    "    WikiHow_row['id'] = f\"wikiHow\"\n",
    "    WikiHow_row['question'] = q['question']\n",
    "        \n",
    "    # Placeholder for explanation or any additional info\n",
    "       # Assuming the correct answer is in q.correct_answer\n",
    "    \n",
    "    options = q['options']\n",
    "    if not options:\n",
    "            options = []\n",
    "            WikiHow_row['answer'] = q['answer']\n",
    "    else:\n",
    "        options=options.split('\\n')\n",
    "        options=[option[3:] for option in options]\n",
    "        WikiHow_row['answer'] = OPTIONS[options.index(q['answer'])]\n",
    "    WikiHow_row['options'] = options\n",
    "    \n",
    "    if q['options_difficulty']:\n",
    "        options_difficulty = int(q['options_difficulty'])\n",
    "    else:\n",
    "        options_difficulty = q['options_difficulty']\n",
    "    WikiHow_row['options_difficulty'] = options_difficulty\n",
    "    # Metadata section: Assuming placeholders for now\n",
    "    \n",
    "    # Placeholder for category if applicable, adjust based on your requirements\n",
    "    WikiHow_row[\"question_template\"] = q['question_type']\n",
    "    \n",
    "    category = wikihow_category_url[q['url']].replace('Gardening/','')\n",
    "    \n",
    "    WikiHow_row['category'] = category.split('/')[0]  # Replace with actual category logic if needed\n",
    "    if len(category.split('/'))>1:\n",
    "        WikiHow_row['subcategory'] = category.split('/')[1]\n",
    "    else:\n",
    "        WikiHow_row['subcategory'] = None\n",
    "    \n",
    "    \n",
    "    #WikiHow_row['context'] = #f\"This question is sourced from the book titled '{book_title}', specifically found in the chapter '{chapter_title}'.\"\n",
    "    \n",
    "    # Placeholder for question_type logic: assuming it's a multiple-choice or open-ended type\n",
    "    if options:\n",
    "        WikiHow_row['question_type'] = 'multiple-choice'\n",
    "    else:\n",
    "        WikiHow_row['question_type'] = 'open-ended'\n",
    "    \n",
    "    \n",
    "    WikiHow_row['metadata']['source'] = 'wikiHow'  # Replace with actual source if needed\n",
    "    WikiHow_row['metadata']['license'] = \"\"\n",
    "    WikiHow_row['metadata']['language'] = 'English'\n",
    "    WikiHow_row['metadata']['url'] = q['url']\n",
    "    WikiHow_row['metadata']['verbose_answer'] = q['answer']\n",
    "    # Appending the row to rows\n",
    "    WikiHow_rows.append(deepcopy(WikiHow_row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 16/16 [00:00<00:00, 2694.49 examples/s]\n",
      "Map:   0%|          | 0/1700 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1700/1700 [00:00<00:00, 6413.01 examples/s]\n",
      "Map: 100%|██████████| 425/425 [00:00<00:00, 8991.21 examples/s]\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/workdir/important_datasets/AGRIVQA/wikiHow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWikiHow\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m root_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/workdir/important_datasets/AGRIVQA\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43msplit_and_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWikiHow_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset_dict(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root_folder, dataset_name))\n\u001b[1;32m      6\u001b[0m dataset\n",
      "Cell \u001b[0;32mIn[2], line 110\u001b[0m, in \u001b[0;36msplit_and_save\u001b[0;34m(all_rows, root_folder, max_len, dataset_name, dev_count, test_ratio, validation_ratio, random_seed)\u001b[0m\n\u001b[1;32m    107\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root_folder, dataset_name, file_name)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Make sure the directory exists\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirname\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Save the chunk to a Parquet file\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSaving dataset file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/workdir/important_datasets/AGRIVQA/wikiHow'"
     ]
    }
   ],
   "source": [
    "dataset_name = 'WikiHow'\n",
    "root_folder = '/workdir/important_datasets/AGRIVQA'\n",
    "split_and_save(WikiHow_rows, root_folder=root_folder)\n",
    "\n",
    "dataset = load_dataset_dict(os.path.join(root_folder, dataset_name))\n",
    "dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
