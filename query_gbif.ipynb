{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING MULTIPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rows Found:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rows Found: 100%|██████████| 1000/1000 [01:56<00:00,  8.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116.52868795394897               gbifID  taxonKey\n",
      "34149     3892886428   2930137\n",
      "45187     3872936011   2930137\n",
      "58122     4431071025   2930137\n",
      "62690     2883212626   2930137\n",
      "88869     4420881430   2930137\n",
      "...              ...       ...\n",
      "14600031  4420512285   2930137\n",
      "14614447  1228033290   2930137\n",
      "14615977   575146991   2930137\n",
      "14615979   574864780   2930137\n",
      "14641706   575014709   2930137\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Value, Lock\n",
    "from tqdm import tqdm\n",
    "import time \n",
    "import csv\n",
    "\n",
    "# Define a global shared variable and lock\n",
    "max_reached = Value('i', 0)  # Shared variable to count rows found\n",
    "lock = Lock()  # Lock to synchronize access to the shared variable\n",
    "\n",
    "def filter_chunk_early_stop(chunk, query, max_num):\n",
    "    \"\"\"Filter rows in a chunk and stop if max_num is reached.\"\"\"\n",
    "    global max_reached, lock\n",
    "    chunk = chunk.dropna(subset=['taxonKey'])\n",
    "    filtered = chunk\n",
    "    for key, value in query.items():\n",
    "        filtered = filtered[filtered[key] == value]\n",
    "        if filtered.empty:\n",
    "            break\n",
    "    \n",
    "    with lock:  # Synchronize access to the shared variable\n",
    "        if max_reached.value >= max_num:\n",
    "            return pd.DataFrame()  # Skip further processing\n",
    "        rows_to_add = min(len(filtered), max_num - max_reached.value)\n",
    "        max_reached.value += rows_to_add\n",
    "        return filtered.head(rows_to_add)\n",
    "\n",
    "def process_file_with_tqdm(file_path, query, max_num, chunksize=100000, num_processes=10):\n",
    "    \"\"\"Filter rows from a large CSV with early stopping using tqdm for progress.\"\"\"\n",
    "    pool = mp.Pool(num_processes)\n",
    "    results = []\n",
    "\n",
    "    # Initialize tqdm progress bar\n",
    "    with tqdm(total=max_num, desc=\"Rows Found\", position=0, leave=True) as pbar:\n",
    "        for chunk in pd.read_csv(file_path, delimiter='\\t', quoting=csv.QUOTE_NONE, chunksize=chunksize, usecols=['gbifID','taxonKey']):\n",
    "            result = pool.apply_async(filter_chunk_early_stop, args=(chunk, query, max_num))\n",
    "            results.append(result)\n",
    "            \n",
    "            # Update progress bar\n",
    "            with lock:\n",
    "                pbar.n = max_reached.value  # Update progress bar to current count\n",
    "                pbar.refresh()  # Refresh the bar to show updated progress\n",
    "            \n",
    "            # Check global variable for early stopping\n",
    "            with lock:\n",
    "                if max_reached.value >= max_num:\n",
    "                    break\n",
    "\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        # Collect results\n",
    "        filtered_results = pd.concat([res.get() for res in results if not res.get().empty])\n",
    "        return filtered_results.head(max_num)\n",
    "\n",
    "# Example usage\n",
    "file_path = '/workdir/datasets/GBIF/occurrence.txt'  # Path to your large CSV file\n",
    "query = {'taxonKey': 2930137}  # Query filters\n",
    "max_num = 1000  # Maximum number of rows to return\n",
    "\n",
    "start = time.time()\n",
    "result = process_file_with_tqdm(file_path, query, max_num)\n",
    "print(time.time() - start, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering Rows (Total Rows: 100/100): : 0it [00:36, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        gbifID                                         identifier\n",
      "0   1228033290  https://iiif.rbge.org.uk/herb/iiif/E00143531/m...\n",
      "1   1228033290  https://iiif.rbge.org.uk/herb/iiif/E00143531/f...\n",
      "2   1668888820  https://inaturalist-open-data.s3.amazonaws.com...\n",
      "3   1668888820  https://inaturalist-open-data.s3.amazonaws.com...\n",
      "4   1668888820  https://inaturalist-open-data.s3.amazonaws.com...\n",
      "..         ...                                                ...\n",
      "95  3112872421  https://inaturalist-open-data.s3.amazonaws.com...\n",
      "96  3113314776  https://inaturalist-open-data.s3.amazonaws.com...\n",
      "97  3301821556  https://inaturalist-open-data.s3.amazonaws.com...\n",
      "98  3301821556  https://inaturalist-open-data.s3.amazonaws.com...\n",
      "99  3302054575  https://inaturalist-open-data.s3.amazonaws.com...\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Manager, Lock\n",
    "import csv\n",
    "\n",
    "manager = Manager()\n",
    "found_list = manager.list()  # Shared list to track found values\n",
    "lock = Lock()  # Lock to synchronize updates\n",
    "\n",
    "def extract_column_values_to_set(df, column_name):\n",
    "    \"\"\"Extract unique values from a specified column and return as a set.\"\"\"\n",
    "    return set(df[column_name].unique())\n",
    "\n",
    "def filter_chunk_limited(chunk, column_name, value_set, max_filt, total_rows):\n",
    "    \"\"\"Filter a chunk and limit results to max_filt rows.\"\"\"\n",
    "    chunk = chunk.dropna(subset=['identifier'])\n",
    "    filtered = chunk[chunk[column_name].astype(int).isin(value_set)]\n",
    "    with lock:\n",
    "        rows_to_add = max_filt - total_rows.value\n",
    "        if rows_to_add <= 0:\n",
    "            return pd.DataFrame(), True  # Signal to stop further processing\n",
    "        limited_filtered = filtered.head(rows_to_add)\n",
    "        total_rows.value += len(limited_filtered)\n",
    "    return limited_filtered, total_rows.value >= max_filt\n",
    "\n",
    "def filter_rows_by_set_limited(file_path, column_name, value_set, chunksize=10000, num_processes=4, max_filt=5000):\n",
    "    \"\"\"Filter rows from a large CSV where columnX is in value_set, limited to max_filt rows.\"\"\"\n",
    "    pool = mp.Pool(num_processes)\n",
    "    results = []\n",
    "    total_rows = manager.Value('i', 0)  # Shared counter for total rows collected\n",
    "    stop_processing = manager.Value('i', 0)  # Flag to stop processing\n",
    "\n",
    "    # Initialize tqdm progress bar\n",
    "    with tqdm(desc=f\"Filtering Rows (Total Rows: 0/{max_filt})\", position=0, leave=True) as pbar:\n",
    "        for chunk in pd.read_csv(file_path, delimiter='\\t', quoting=csv.QUOTE_NONE, chunksize=chunksize, usecols=['gbifID', 'identifier']):\n",
    "            if stop_processing.value:  # Check if we should stop processing\n",
    "                break\n",
    "            \n",
    "            result = pool.apply_async(filter_chunk_limited, args=(chunk, column_name, value_set, max_filt, total_rows))\n",
    "            results.append(result)\n",
    "            \n",
    "            with lock:\n",
    "                pbar.set_description(f\"Filtering Rows (Total Rows: {total_rows.value}/{max_filt})\")\n",
    "                pbar.refresh()\n",
    "                if total_rows.value >= max_filt:\n",
    "                    stop_processing.value = 1\n",
    "                    break\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # Combine results\n",
    "    filtered_results = pd.concat([res.get()[0] for res in results if not res.get()[0].empty], ignore_index=True)\n",
    "    return filtered_results\n",
    "\n",
    "# Example usage\n",
    "\n",
    "# Step 1: Extract 'columnX' values from the 'result' DataFrame into a set\n",
    "columnX = 'gbifID'  # Replace with your actual column name\n",
    "S = extract_column_values_to_set(result, columnX)\n",
    "\n",
    "# Step 2: Read another CSV and filter rows where 'columnX' is in set S, limited to max_filt rows\n",
    "second_file_path = '/workdir/datasets/GBIF/multimedia.txt'  # Path to the second CSV file\n",
    "max_filt = 100  # Limit to 5000 rows\n",
    "filtered_rows = filter_rows_by_set_limited(second_file_path, columnX, S, max_filt=max_filt)\n",
    "\n",
    "# Display the filtered rows\n",
    "print(filtered_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 100 images to /workdir/download/taxonKey/2930137\n"
     ]
    }
   ],
   "source": [
    "# async download images from filtered_rows identifier column save images in save_path \n",
    "import os \n",
    "import requests\n",
    "\n",
    "def download_image(url, save_path):\n",
    "    \"\"\"Download an image from a URL and save it to the specified path.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    with open(save_path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "        \n",
    "def download_images_async(filtered_rows, save_dir):\n",
    "    \"\"\"Download images from URLs in the 'identifier' column of filtered_rows.\"\"\"\n",
    "    urls = filtered_rows['identifier'].tolist()\n",
    "    names = filtered_rows['gbifID'].tolist()\n",
    "    save_paths = [os.path.join(save_dir, f\"{names[i]}.jpg\") for i in range(len(urls))]\n",
    "    with mp.Pool(10) as pool:\n",
    "        pool.starmap(download_image, zip(urls, save_paths))\n",
    "        \n",
    "# Example usage\n",
    "save_dir = '/workdir/download/'+'/'.join(sum(list([k,str(v)] for k,v in query.items()),[]))  # Directory to save images\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "download_images_async(filtered_rows, save_dir)\n",
    "print(f\"Downloaded {len(filtered_rows)} images to {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7 (from pyspark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840625 sha256=8c26d1ad0721feaab24fbf03f8a0002c6eb2f471d8451f8f7b7288394e6e189b\n",
      "  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark[sql] in /root/miniconda3/lib/python3.10/site-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /root/miniconda3/lib/python3.10/site-packages (from pyspark[sql]) (0.10.9.7)\n",
      "Requirement already satisfied: pandas>=1.0.5 in /root/miniconda3/lib/python3.10/site-packages (from pyspark[sql]) (2.2.3)\n",
      "Requirement already satisfied: pyarrow>=4.0.0 in /root/miniconda3/lib/python3.10/site-packages (from pyspark[sql]) (18.1.0)\n",
      "Requirement already satisfied: numpy<2,>=1.15 in /root/miniconda3/lib/python3.10/site-packages (from pyspark[sql]) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.10/site-packages (from pandas>=1.0.5->pyspark[sql]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.10/site-packages (from pandas>=1.0.5->pyspark[sql]) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.10/site-packages (from pandas>=1.0.5->pyspark[sql]) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->pyspark[sql]) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting plotly\n",
      "  Downloading plotly-5.24.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pyspark[pandas_on_spark] in /root/miniconda3/lib/python3.10/site-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /root/miniconda3/lib/python3.10/site-packages (from pyspark[pandas_on_spark]) (0.10.9.7)\n",
      "Requirement already satisfied: pandas>=1.0.5 in /root/miniconda3/lib/python3.10/site-packages (from pyspark[pandas_on_spark]) (2.2.3)\n",
      "Requirement already satisfied: pyarrow>=4.0.0 in /root/miniconda3/lib/python3.10/site-packages (from pyspark[pandas_on_spark]) (18.1.0)\n",
      "Requirement already satisfied: numpy<2,>=1.15 in /root/miniconda3/lib/python3.10/site-packages (from pyspark[pandas_on_spark]) (1.26.4)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /root/miniconda3/lib/python3.10/site-packages (from plotly) (9.0.0)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.10/site-packages (from plotly) (24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.10/site-packages (from pandas>=1.0.5->pyspark[pandas_on_spark]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.10/site-packages (from pandas>=1.0.5->pyspark[pandas_on_spark]) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.10/site-packages (from pandas>=1.0.5->pyspark[pandas_on_spark]) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->pyspark[pandas_on_spark]) (1.16.0)\n",
      "Downloading plotly-5.24.1-py3-none-any.whl (19.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: plotly\n",
      "Successfully installed plotly-5.24.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pyspark[connect] in /root/miniconda3/lib/python3.10/site-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /root/miniconda3/lib/python3.10/site-packages (from pyspark[connect]) (0.10.9.7)\n",
      "Requirement already satisfied: pandas>=1.0.5 in /root/miniconda3/lib/python3.10/site-packages (from pyspark[connect]) (2.2.3)\n",
      "Requirement already satisfied: pyarrow>=4.0.0 in /root/miniconda3/lib/python3.10/site-packages (from pyspark[connect]) (18.1.0)\n",
      "Collecting grpcio>=1.56.0 (from pyspark[connect])\n",
      "  Downloading grpcio-1.68.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting grpcio-status>=1.56.0 (from pyspark[connect])\n",
      "  Downloading grpcio_status-1.68.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting googleapis-common-protos>=1.56.4 (from pyspark[connect])\n",
      "  Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.15 in /root/miniconda3/lib/python3.10/site-packages (from pyspark[connect]) (1.26.4)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2 in /root/miniconda3/lib/python3.10/site-packages (from googleapis-common-protos>=1.56.4->pyspark[connect]) (5.28.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.10/site-packages (from pandas>=1.0.5->pyspark[connect]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.10/site-packages (from pandas>=1.0.5->pyspark[connect]) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.10/site-packages (from pandas>=1.0.5->pyspark[connect]) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->pyspark[connect]) (1.16.0)\n",
      "Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl (221 kB)\n",
      "Downloading grpcio-1.68.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio_status-1.68.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: grpcio, googleapis-common-protos, grpcio-status\n",
      "Successfully installed googleapis-common-protos-1.66.0 grpcio-1.68.0 grpcio-status-1.68.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Spark SQL\n",
    "! pip install pyspark[sql]\n",
    "# pandas API on Spark\n",
    "! pip install pyspark[pandas_on_spark] plotly  # to plot your data, you can install plotly together.\n",
    "# Spark Connect\n",
    "! pip install pyspark[connect]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USING PYSPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/29 16:02:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/workdir/datasets/GBIF/occurrence.txt'\n",
    "df = spark.read.options(delimiter=\"\\t\", header=True).csv(path)\n",
    "path = '/workdir/datasets/GBIF/multimedia.txt'\n",
    "dfm = spark.read.options(delimiter=\"\\t\", header=True).csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/29 16:02:41 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"occurrence\")\n",
    "dfm.createOrReplaceTempView(\"multimedia\")\n",
    "\n",
    "# select 1000 gbifID from occurrence where taxonKey = 2930137 and take 100 rows from multimedia having gbifID in the result\n",
    "sqlDF = spark.sql(\"SELECT gbifID FROM occurrence WHERE taxonKey = 2930137 LIMIT 1000\")\n",
    "sqlDF.createOrReplaceTempView(\"occurrence_filtered\")\n",
    "sqlDF = spark.sql(\"SELECT gbifID,identifier FROM multimedia WHERE gbifID in (SELECT gbifID FROM occurrence_filtered) LIMIT 200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = sqlDF.collect()\n",
    "dfr = spark.createDataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "query = {'taxonKey': 2930137}  # Query filters\n",
    "save_dir = '/workdir/download/'+'/'.join(sum(list([k,str(v)] for k,v in query.items()),[]))  # Directory to save images\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "download_images_async(filtered_rows, save_dir)\n",
    "print(f\"Downloaded {len(filtered_rows)} images to {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
